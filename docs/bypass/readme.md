# **Bypasser l'anti-scrawling**<a href="../../"><img align="right" src="../../assets/atomicWebScraping.png" alt="Web scraping" height="64px"></a>
Il y a règulièrement de nouvelles techniques utilisées pour éviter d'être détecté et bloqué lors de l'extraction de données par les sites web [1][2]. Les sites web mettent en œuvre diverses mesures anti-scraping pour protéger leurs données, préserver les ressources du serveur et maintenir un avantage concurrentiel [1]. Comprendre ces mesures et savoir comment les contourner est essentiel pour réussir le web scraping [1][2].

1. [La rotation d'adresses IP](ipRotation)
1. [Les serveurs proxy](proxy)
1. [Les délais et limitations de requêtes](rateLimiting)
1. [Le contournement des CAPTCHA](captchaBypass)
1. [La gestion des honeypots](honeypots)
1. [Les navigateurs sans tête (Headless Browsers)](headlessBrowsers)
1. [L'analyse du comportement de l'utilisateur (UBA)](userBehaviorAnalysis)
1. [Les User-Agents](userAgents)
1. [Les cookies et les sessions](cookiesSessions)

⚠️ Il est important de noter que le web scraping doit être effectué de manière éthique et en respectant les conditions d'utilisation des sites web [4][5]. Le contournement des mesures anti-scraping ne doit pas être utilisé pour surcharger les serveurs ou enfreindre les lois sur la protection des données [4].
___
Références :
[1] https://www.scrapingbee.com/blog/best-anti-scraping-techniques/
[2] https://www.scraperapi.com/blog/how-to-bypass-anti-scraping-techniques/
[3] https://proxyscrape.com/blog/best-ways-to-bypass-anti-scraping-measures/
[4] https://www.oxfordlawtrove.com/view/10.1093/he/9780198840602.001.0001/he-9780198840602-chapter-9/
[5] https://www.eff.org/deeplinks/2017/04/ethical-web-scraping-guide/