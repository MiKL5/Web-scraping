# **Bypass - Contournement Anti-Bot**<a href="../../"><img align="right" src="../../assets/atomicWebScraping.png" alt="Web scraping" height="64px"></a>

<div align="center">

![Python](https://img.shields.io/badge/python-3.13-blue?style=flat&logo=python&logoColor=FFD43B) 
![Scrapy](https://img.shields.io/badge/Scrapy-2.13.4-5E8862?style=flat&logo=scrapy&logoColor=white) 
![Cloudflare](https://img.shields.io/badge/Cloudflare-Bypass-EA580C?style=flat&logo=cloudflare&logoColor=white) 
![curl_cffi](https://img.shields.io/badge/curl_cffi-TLS_Fingerprint-059669?style=flat&logo=curl&logoColor=white) 
![Selenium](https://img.shields.io/badge/Selenium-Stealth-43B02A?style=flat&logo=selenium&logoColor=white) 
![Playwright](https://img.shields.io/badge/Playwright-Anti_Detect-0EA5E9?style=flat&logo=playwright&logoColor=white)
![User Agents](https://img.shields.io/badge/User_Agents-4531+-8B5CF6?style=flat&logo=internetexplorer&logoColor=white)

</div>

<hr>

**Bypass** est un framework Scrapy con√ßu pour contourner les protections anti-scraping avec des techniques de stealth scraping, rotation dynamique de User-Agents, TLS fingerprinting et simulation de comportement humain.
---
## ‚ú® **Les fonctionnalit√©s**
### üîÑ **Rotation intelligente de User-Agents**
- **4531+ User-Agents** charg√©s dynamiquement depuis GitHub
- Rotation automatique √† **chaque requ√™te**
- Support de Safari, Firefox, Chrome et Opera
- Chargement avec rotation (chaque liste est t√©l√©charg√©e avec un UA diff√©rent)
### üõ°Ô∏è **Protection anti-d√©tection**
- Middleware de rotation automatique des User-Agents
- Support du TLS fingerprinting avec `curl_cffi`
- Headers HTTP r√©alistes
- Gestion des cookies intelligente
### üéØ **Performance optimis√©e**
- Requ√™tes concurrentes configurables
- Syst√®me de retry intelligent
- Cache HTTP optionnel
- Timeout configurable
### üìä **Monitoring et logging**
- Logs d√©taill√©s des User-Agents utilis√©s
- Statistiques de scraping
- Support Telnet pour monitoring en temps r√©el
## üèóÔ∏è **L'architecture du projet**
```
bypass/
‚îú‚îÄ‚îÄ __init__.py                 # Initialisation du package
‚îú‚îÄ‚îÄ settings.py                 # Configuration principale + chargement User-Agents
‚îú‚îÄ‚îÄ middlewares.py              # Middlewares personnalis√©s
‚îú‚îÄ‚îÄ items.py                    # D√©finition des items Scrapy
‚îú‚îÄ‚îÄ pipelines.py                # Pipelines de traitement
‚îú‚îÄ‚îÄ spiders/                    # Dossier des spiders
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ example_spider.py       # Spider d'exemple
‚îÇ   ‚îî‚îÄ‚îÄ ...                     # Vos spiders personnalis√©s
‚îú‚îÄ‚îÄ scrapy.cfg                  # Configuration Scrapy
‚îî‚îÄ‚îÄ README.md                   # Documentation
```
### üì¶ **Les principaux composants**
#### `settings.py`
Configuration centrale du projet incluant :
* Chargement dynamique des User-Agents depuis 4 sources GitHub
* Configuration Scrapy (delays, concurrency, retries)
* Fonction `get_random_user_agent()` exportable
#### `middlewares.py`
Deux middlewares personnalis√©s :
* **`RandomUserAgentMiddleware`** : Change le User-Agent √† chaque requ√™te
* **`BypassSpiderMiddleware`** : Middleware Spider optionnel
#### `spiders/`
Contient tous vos spiders de scraping. Chaque spider h√©rite de `scrapy.Spider` et utilise automatiquement la rotation de User-Agents.
## üîê **Les techniques de contournement**
### 1. **Rotation de User-Agents**
```python
# Chargement depuis GitHub avec rotation
urls = [
    "Safari.txt",    # 596 User-Agents
    "Firefox.txt",   # 2104 User-Agents
    "Chrome.txt",    # 852 User-Agents
    "Opera.txt"      # 992 User-Agents
]
# Total : 4531 User-Agents uniques
```
**Fonctionnement** :
1. Chaque liste est t√©l√©charg√©e avec un User-Agent diff√©rent
2. Les User-Agents sont stock√©s dans un pool global
3. √Ä chaque requ√™te, un UA al√©atoire est s√©lectionn√©
4. √âvite la d√©tection par patterns r√©p√©titifs
### 2. **TLS Fingerprinting (curl_cffi)**
```python
# Imitation de navigateurs r√©els au niveau TLS
impersonate = "chrome110"  # ou firefox, safari, edge
```
### 3. **Headers r√©alistes**
```python
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}
```
### 4. **Les delays et comportement humain**
```python
DOWNLOAD_DELAY = 1  # D√©lai entre requ√™tes
RANDOMIZE_DOWNLOAD_DELAY = True  # Variation al√©atoire
CONCURRENT_REQUESTS_PER_DOMAIN = 1  # √âvite la surcharge
```
### 5. **Le syst√®me de retry intelligent**
```python
RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]
```
### 6. **La gestion des cookies**
```python
COOKIES_ENABLED = True
COOKIES_DEBUG = False
```
## üì• **L'istallation**
### Les pr√©requis
`Python 3.13+` et `pip`
### Installer les d√©pendances
```bash
# Cr√©er un environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows
# Installer les packages
pip install scrapy requests curl_cffi selenium playwright
```
### Installer (optionnelement) Playwright
```bash
playwright install chromium
```
## ‚öôÔ∏è **La configuration**
### `settings.py` - Configuration principale
```python
# D√©lai entre requ√™tes
DOWNLOAD_DELAY = 1

# Requ√™tes concurrentes
CONCURRENT_REQUESTS = 16
CONCURRENT_REQUESTS_PER_DOMAIN = 1

# Timeout
DOWNLOAD_TIMEOUT = 30

# Log level
LOG_LEVEL = 'INFO'  # DEBUG pour plus de d√©tails

# Middlewares
DOWNLOADER_MIDDLEWARES = {
    'bypass.middlewares.RandomUserAgentMiddleware': 400,
}
```
### Les variables d'environnement (optionnel)
Cr√©ez un fichier `.env` :
```env
SCRAPY_LOG_LEVEL=INFO
SCRAPY_DOWNLOAD_DELAY=1
SCRAPY_CONCURRENT_REQUESTS=16
```
## üöÄ **L'utilisation**
### D√©marrer le spider
```bash
# Spider basique
scrapy crawl bypass

# Avec output JSON
scrapy crawl bypass -O output.json

# Avec output CSV
scrapy crawl bypass -O output.csv

# Avec log d√©taill√©
scrapy crawl bypass -L DEBUG
```
### Tester la rotation de User-Agents
```bash
# Lancer settings.py directement
python bypass/settings.py
```
Output attendu :
```
‚úì Charg√© 596 User-Agents depuis Safari.txt
‚úì Charg√© 2104 User-Agents depuis Firefox.txt
‚úì Charg√© 852 User-Agents depuis Chrome.txt
‚úì Charg√© 992 User-Agents depuis Opera.txt

üìä Il y a 4531 User-Agents uniques charg√©s

[Requ√™te 1] User-Agent utilis√©: Mozilla/5.0 (Windows NT 10.0...)
[Requ√™te 2] User-Agent utilis√©: Mozilla/5.0 (Macintosh; Intel...)
```
## üì¶ **Les d√©pendances**
### Packages Python
```bash
scrapy>=2.13.4
requests>=2.31.0
curl_cffi>=0.6.0
selenium>=4.15.0
playwright>=1.40.0
lxml>=5.0.0
```
### L'installation compl√®te
```bash
pip install -r requirements.txt
```
**requirements.txt** :
```txt
scrapy==2.13.4
requests==2.32.3
curl-cffi==0.7.4
selenium==4.27.1
playwright==1.49.1
lxml==6.0.2
parsel==1.10.0
twisted==25.5.0
pyOpenSSL==25.3.0
cryptography==46.0.3
```
## üõ°Ô∏è **Les bonnes pratiques**
‚úÖ √Ä faire | ‚ùå √Ä √©viter
---|---
Respecter les d√©lais entre requ√™tes (`DOWNLOAD_DELAY`) | Scraper trop rapidement (risque de ban IP)
Utiliser un User-Agent diff√©rent √† chaque requ√™te | Utiliser le m√™me User-Agent partout
V√©rifier le `robots.txt` avant de scraper | Ignorer les codes d'erreur 429 (Too Many Requests)
Impl√©menter un syst√®me de retry | Scraper des donn√©es sensibles sans autorisation
Logger les erreurs et succ√®s | N√©gliger la gestion des cookies
## üìù **Licence**
Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

<hr><div align="center">

**‚≠ê Si ce projet vous aide, n'h√©sitez pas √† lui donner une √©toile !**

Made with ‚ù§Ô∏è and Scrapy
