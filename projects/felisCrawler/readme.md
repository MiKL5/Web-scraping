# **FelisCrawler**<a href="../../"><img align="right" src="../../assets/atomicWebScraping.png" alt="Web scraping" height="64px"></a>
<div align="center">

![Python](https://img.shields.io/badge/python-3.13-blue?style=flat&logo=python&logoColor=FFD43B) 
![Scrapy](https://img.shields.io/badge/Scrapy-Web_Scraping-5E8862?style=flat&logo=scrapy&logoColor=white) 
![Streamlit](https://img.shields.io/badge/Streamlit-UI_Interactive-FF4B4B?style=flat&logo=streamlit&logoColor=white) 
![Pandas](https://img.shields.io/badge/pandas-Data_Processing-150458?style=flat&logo=pandas&logoColor=white) 
![JSON](https://img.shields.io/badge/JSON-Export-000000?style=flat&logo=json&logoColor=white) 
![csv](https://img.shields.io/badge/CSV-Export-000000?style=flat)

</div>

"**FelisCrawler**" est une application de web scraping pÃ©dagogique. Elle explore automatiquement les articles WikipÃ©dia liÃ©s aux chats. Elle combine un spider Scrapy performant avec une interface Streamlit interactive pour configurer, exÃ©cuter et visualiser vos extractions de donnÃ©es.
---
**ğŸ¯ Cas d'usage** ğŸ‘‰ Apprentissage du web scraping, analyse de contenu encyclopÃ©dique, Ã©tude de graphes de liens.
___

<a href="#"><img align="right" src="../../assets/felisCrawlerr.png" alt="FelisCrwler"></a>

## **Les principaux composants du projet sont**
* Un spider Scrapy avancÃ© multi rÃ¨gles, conÃ§u pour explorer en profondeur les liens encyclopÃ©diques autour des chats sur WikipÃ©dia.
* L'interface utilisateur graphique avec Streamlit. Elle centralise la configuration, le lancement du scraping, la visualisation des rÃ©sultats (statistiques, tableaux, graphiques) et la documentation Ã©thique.
## **Les principales fonctionnalitÃ©s sont**
* Un scraping exhaustif de WikipÃ©dia sur la thÃ©matique fÃ©line (avec le suivi intelligent des liens pertinents et filtrage)
* Le contrÃ´le total des paramÃ¨tres
  * La profondeur de crawl
  * Le dÃ©lai minimum entre requÃªtes (pour le respect des serveurs)
  * Le nombre de requÃªtes simultanÃ©es
  * Le nom du fichier de sortie.

* La visualisation instantanÃ©e des rÃ©sultats du scraping
  * La quantitÃ© de pages, paragraphes, images, liens internes
  * Le tableau filtrable des articles
  * Les fiches dÃ©taillÃ©es par article (titre, intro, liens internes, images) ;
  * Les graphiques interactifs
    * La rÃ©partition par profondeur, 
    * La distribution des paragraphes,
    * La relation longueur/nombre dâ€™images.

* Le tÃ©lÃ©chargement Ã  la demande des rÃ©sultats en JSON
* Une documentation sur lâ€™Ã©thique, lâ€™impact environnemental du scraping, le RGPD et les bonnes pratiques
## **L'architecture du projet**
```
felisCrawler/
â”œâ”€â”€ app.py                             # Interface Streamlit principale
â”œâ”€â”€ utils.py                           # Fonctions utilitaires
â”œâ”€â”€ run_tests.py                       # Script de lancement des tests
â”œâ”€â”€ wikipedia/
â”‚   â””â”€â”€ spiders/
â”‚       â””â”€â”€ feliscrawler_spider.py     # Spider Scrapy (CrawlSpider)
â”œâ”€â”€ tests/                             # Suite de tests (25 tests, 86% couverture)
â”‚   â”œâ”€â”€ test_integrity.py              # Tests d'extraction de donnÃ©es
â”‚   â”œâ”€â”€ test_structure.py              # Tests de structure HTML (live)
â”‚   â”œâ”€â”€ test_live_structure.py         # Tests live (flag --live)
â”‚   â”œâ”€â”€ test_edge_cases.py             # Tests de cas limites
â”‚   â”œâ”€â”€ test_navigation.py             # Tests de filtrage des liens
â”‚   â”œâ”€â”€ test_e2e.py                    # Tests end-to-end
â”‚   â”œâ”€â”€ test_app_ui.py                 # Tests de l'interface Streamlit
â”‚   â”œâ”€â”€ test_components.py             # Tests des composants UI
â”‚   â”œâ”€â”€ test_settings.py               # Tests de configuration
â”‚   â””â”€â”€ test_utils.py                  # Tests des utilitaires
â”œâ”€â”€ requirements.txt                   # DÃ©pendances Python
â”œâ”€â”€ pyproject.toml                     # Configuration projet
â”œâ”€â”€ scrapy.cfg                         # Configuration Scrapy
â””â”€â”€ readme.md                          # Cette documentation
```
**Le spider Scrapy** scrute `fr.wikipedia.org/wiki/Chat` en suivant des rÃ¨gles d'exploration intelligentes :
* Suit uniquement les URL pertinentes (chats, fÃ©lins, races)
* Exclut les espaces non encyclopÃ©diques (pages meta, discussions, etc.)
* Extrait pour chaque page : titre, introduction, nombre de paragraphes, longueur texte, liens internes, images filtrÃ©es et profondeur

**L'interface Streamlit** centralise toutes les fonctionnalitÃ©s :
* Configuration intuitive via la sidebar
* Lancement du scraping en un clic (gÃ©nÃ¨re un sous-processus Scrapy)
* Visualisation interactive avec filtrage et recherche
* Statistiques et graphiques dynamiques
* Documentation sur l'Ã©thique et la gouvernance du scraping
## **ğŸš€ DÃ©marrage rapide**
**PrÃ©requis** ğŸ‘‰ Python 3.9 ou ultÃ©rieur (testÃ© avec Python 3.13)
### **Ã‰tape 1 ğŸ‘‰ Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```
### **Ã‰tape 2 ğŸ‘‰ DÃ©marrer l'application**
```sh
streamlit run app.py
```
### **Ã‰tape 3 ğŸ‘‰ Scraper**
1. ParamÃ©trer les options dans la barre latÃ©rale (profondeur, dÃ©lai, etc.)
2. Cliquer sur **"ğŸš€ Je scrape !"**
3. Les rÃ©sultats apparaissent automatiquement dans `result.json` et `result.csv`
### **Depuis l'interface**, les actions sont
* Configurer les paramÃ¨tres de scraping (profondeur, dÃ©lai, concurrence)
* Lancer le scraping en un clic
* Explorer les donnÃ©es collectÃ©es dans les onglets interactifs
* Filtrer et rechercher dans les rÃ©sultats
* Exporter au format JSON et CSV
## **Les spÃ©cificitÃ©s du spider**
* Configuration fine par attributs Scrapy (profondeur, dÃ©lais, concurrent, User-Agent)
* Respect du fichier '_robots.txt_'
* Extraction robuste des titres, introduction et paragraphes
* Limitation intelligente des liens internes et images extraites pour Ã©viter les dÃ©bordements
* Compatible avec tous les formats dâ€™export supportÃ©s (FEEDS Scrapy)
## **Visualiser et analyser**
L'interface propose plusieurs outils d'analyse
* **Un tableau interactif** ğŸ‘‰ Filtrer les pages par profondeur, nombre d'images, longueur ou recherche textuelle dans les titres.
* **Des fiches dÃ©taillÃ©es** ğŸ‘‰ Pour chaque article, consulter le titre, l'URL, l'introduction, les liens internes et un aperÃ§u des images.
* **Des graphiques dynamiques**
    * RÃ©partition des pages par profondeur de crawl (bar chart)
    * Relation longueur de texte / nombre d'images (scatter plot)
    * Distribution du nombre de paragraphes par page (histogramme)

<details>
<summary>Exemple de donnÃ©es extraites</summary>

```json
{
    "url": "https://fr.wikipedia.org/wiki/Chat",
    "titre": "Chat",
    "profondeur": 0,
    "introduction": "Le Chat domestique (Felis silvestris catus) est la sous-espÃ¨ce...",
    "nombre_paragraphes": 142,
    "longueur_contenu": 45231,
    "nombre_images": 38,
    "liens_internes": [
        "https://fr.wikipedia.org/wiki/Felis_silvestris",
        "https://fr.wikipedia.org/wiki/FÃ©lins",
        "https://fr.wikipedia.org/wiki/Domestication_du_chat"
    ],
    "images": [
        "//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Cat_poster_1.jpg/260px-Cat_poster_1.jpg",
        "//upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Kittyply_edit1.jpg/220px-Kittyply_edit1.jpg"
    ]
}
```
</details>

## **Ã‰thique, gouvernance et bonnes pratiques**
Le scraping limitÃ© Ã  un usage expÃ©rimental et pÃ©dagogique. Il respecte explicitement les rÃ¨gles WikipÃ©dia (licence CC BY-SA, attribution requise). Il n'y a pas de collecte de donnÃ©es personnelles dans le pÃ©rimÃ¨tre du projet. 
Le respect de lâ€™environnement est possible par la modulation des paramÃ¨tres pour minimiser lâ€™impact carbone et Ã©viter de surcharger le serveur.
## **Licence**
* **Le code** ğŸ‘‰ [voir le fichier `LICENSE`](LICENSE) Ã  la racine du projet pour la licence du code source.
* **Les donnÃ©es extraites** depuis WikipÃ©dia sont soumises Ã  [la licence WikipÃ©dia `CC BYâ€‘SA`](https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Droit_d%27auteur).
___

![screenshot](assets/screenshot.png)
![screenshot1](assets/screenshot1.png)
___
## **Les tests**
Pour garantir la pÃ©rennitÃ© du scraper face aux Ã©volutions de WikipÃ©dia, une **suite de tests complÃ¨te** est incluse.
<!--### **RÃ©sumÃ©**
* **48 tests** rÃ©ussis (100% de rÃ©ussite)
* **90%** de couverture globale
* **12 types de tests** : intÃ©gritÃ©, structure, cas limites, navigation, end-to-end, UI, middlewares, coverage, etc.
### **Commandes rapides**
```bash
# Tous les tests
python run_tests.py
# Tests avec couverture
coverage run -m unittest discover && coverage report -m
# Tests live (rÃ©seau requis)
python run_tests.py --live
# Analyse statique
ruff check .
mypy .
```-->
Pour plus de dÃ©tails sur chaque type de test, exemples, troubleshooting et guide de contribution aux tests

ğŸ‘‰ **[Consulter testing.md](testing.md)**

<details>
<summary>Les problÃ¨mes courants</summary><hr>

#### **Erreur ğŸ‘‰ `ModuleNotFoundError: No module named 'scrapy'`**
#### **Solution** ğŸ‘‰ Installez les dÃ©pendances avec `pip install -r requirements.txt`
#### **Aucune donnÃ©e scrapÃ©e / Fichier JSON vide**
#### **Causes possibles**
* Connexion internet indisponible
* Wikipedia est inaccessible
* Les sÃ©lecteurs CSS/XPath ont changÃ© (structure HTML modifiÃ©e)
#### **Solution** ğŸ‘‰ Lancer `python run_tests.py` pour vÃ©rifier si les tests de structure dÃ©tectent des changements
#### **Tests Ã©chouent**
#### **Solution** 
* Utiliser `python run_tests.py` pour la suite complÃ¨te sans tests rÃ©seau
* Utiliser `python run_tests.py --live` pour inclure les tests nÃ©cessitant une connexion internet
* Avoir Python 3.9+ ğŸ‘‰ `python --version`
#### **L'application Streamlit ne dÃ©marre pas**
#### **Solution** 
* VÃ©rifier que Streamlit est installÃ© ğŸ‘‰ `pip show streamlit`
* RÃ©installer si nÃ©cessaire ğŸ‘‰ `pip install --upgrade streamlit`
* Utiliser la commande complÃ¨te ğŸ‘‰ `streamlit run app.py`
#### **Erreur RGPD / Robots.txt**
Le spider respecte automatiquement `robots.txt` grÃ¢ce Ã  `ROBOTSTXT_OBEY=True`. Si vous rencontrez des blocages, augmentez le dÃ©lai entre requÃªtes dans l'interface.

</details><hr>

## **RÃ©fÃ©rences et documentation**
[Scrapy â€” Spiders](https://docs.scrapy.org/en/latest/topics/spiders.html)  
[Scrapy â€” SÃ©lecteurs XPath](https://docs.scrapy.org/en/latest/topics/selectors.html)  
[Scrapy â€” ParamÃ¨tres de pipeline et exports](https://docs.scrapy.org/en/latest/topics/feed-exports.html)  
[Documentation de Streamlit](https://docs.streamlit.io/)  
[Licences et conditions dâ€™utilisation de WikipÃ©dia](https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Droit_d%27auteur)  
[RÃ©capitulatif du RGPD](https://www.cnil.fr/fr/rgpd-par-ou-commencer)

<hr><div align="center">

**FelisCrawler** est conÃ§u pour illustrer lâ€™intÃ©gration de technologies de scraping et dâ€™interface utilisateur dans un contexte pÃ©dagogique et gouvernÃ©.