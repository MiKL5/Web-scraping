# **FelisCrawler**<a href="../../"><img align="right" src="../../assets/atomicWebScraping.png" alt="Web scraping" height="64px"></a>
<div align="center">

![Python](https://img.shields.io/badge/python-3.13-blue?style=flat&logo=python&logoColor=FFD43B) 
![Scrapy](https://img.shields.io/badge/Scrapy-Web_Scraping-5E8862?style=flat&logo=scrapy&logoColor=white) 
![Streamlit](https://img.shields.io/badge/Streamlit-UI_Interactive-FF4B4B?style=flat&logo=streamlit&logoColor=white) 
![Pandas](https://img.shields.io/badge/pandas-Data_Processing-150458?style=flat&logo=pandas&logoColor=white) 
![JSON](https://img.shields.io/badge/JSON-Export-000000?style=flat&logo=json&logoColor=white) 
![csv](https://img.shields.io/badge/CSV-Export-000000?style=flat)

</div>

"**_FelisCrawler_**" est une application de scraping. Elle permet de
scraper tous les articles Wikip√©dia (en fran√ßais) li√©s aux chats. Visualiser et analyser interactivement le contenu structur√© issu du scraping. Et de piloter le scraping et exporter les donn√©es simplement.
---
## **Les principaux composants du projet sont**
* Un spider Scrapy avanc√© multi r√®gles, con√ßu pour explorer en profondeur les liens encyclop√©diques autour des chats sur Wikip√©dia.
* L'interface utilisateur graphique avec Streamlit. Elle centralise la configuration, le lancement du scraping, la visualisation des r√©sultats (statistiques, tableaux, graphiques) et la documentation √©thique.
## **Les principales fonctionnalit√©s sont**
* Un scraping exhaustif de Wikip√©dia sur la th√©matique f√©line (avec le suivi intelligent des liens pertinents et filtrage)
* Le contr√¥le total des param√®tres
  * La profondeur de crawl
  * Le d√©lai minimum entre requ√™tes (pour le respect des serveurs)
  * Le nombre de requ√™tes simultan√©es
  * Le nom du fichier de sortie.

* La visualisation instantan√©e des r√©sultats du scraping
  * La quantit√© de pages, paragraphes, images, liens internes
  * Le tableau filtrable des articles
  * Les fiches d√©taill√©es par article (titre, intro, liens internes, images) ;
  * Les graphiques interactifs
    * La r√©partition par profondeur, 
    * La distribution des paragraphes,
    * La relation longueur/nombre d‚Äôimages.

* Le t√©l√©chargement √† la demande des r√©sultats en JSON
* Une documentation sur l‚Äô√©thique, l‚Äôimpact environnemental du scraping, le RGPD et les bonnes pratiques
## **Organisation des fichiers**
**Un CrawlSpider scrutant fr.wikipedia.org/wiki/Chat, en suivant des r√®gles d‚Äôexploration**
* Suivre des URL pertinentes
* Exclure les espaces non encyclop√©diques
* Extraire √† chaque page : le titre, l'introduction, le nombre paragraphes, la longueur texte, les liens internes, les images filtr√©es et la profondeur.
**L'application permet**
* La configuration intuitive par la sidebar
* Le lancement du scraping en un clic (g√©n√®re un sous-processus Scrapy configurable)
* La visualisation, le filtrage et l'export interactifs des r√©sultats
* Les statistiques et graphiques
* Une rubrique sur l‚Äô√©thique, l‚Äôenvironnement, le droit et la gouvernance du scraping.
## **Pour l'utiliser**
Il faut Python 3.9 ou ult√©rieur. Scrapy, Streamlit et Pandas.
### **Installer les d√©pendances**
```bash
pip install -r requirements.txt
```
### **D√©marer l'appli**
```sh
streamlit run app.py
```
### **Depuis l‚Äôinterface dans un navigateur**, vous pouvez
* Param√©trer les options de scraping dans la barre lat√©rale
* Lancer le scraping
* Explorer les donn√©es collect√©es dans les onglets
* Filtrer, rechercher, et exporter les r√©sultats au format JSON et CSV
## **Les sp√©cificit√©s du spider**
* Configuration fine par attributs Scrapy (profondeur, d√©lais, concurrent, User-Agent)
* Respect du fichier '_robots.txt_'
* Extraction robuste des titres, introduction et paragraphes
* Limitation intelligente des liens internes et images extraites pour √©viter les d√©bordements
* Compatible avec tous les formats d‚Äôexport support√©s (FEEDS Scrapy)
## **Visualiser et analyser**
Il y a un tableau filtrable pour s√©lectionner les pages par profondeur, nombre d‚Äôimages, longueur, titre. Les d√©tails de chaque page.  
Des graphiques concernant
* La r√©partition des pages par profondeur de crawl
* La relation longueur de texte/nombre d‚Äôimages (scatter)
* La distribution du nombre de paragraphes par page
## **√âthique, gouvernance et bonnes pratiques**
Le scraping limit√© √† un usage exp√©rimental et p√©dagogique. Il respecte explicitement les r√®gles Wikip√©dia (licence CC BY-SA, attribution requise). Il n'y a pas de collecte de donn√©es personnelles dans le p√©rim√®tre du projet. 
Le respect de l‚Äôenvironnement est possible par la modulation des param√®tres pour minimiser l‚Äôimpact carbone et √©viter de surcharger le serveur.

![screenshot](assets/screenshot.png)
![screenshot1](assets/screenshot1.png)

## **Tester et Valider**
Pour garantir la p√©rennit√© du scraper face aux √©volutions de Wikip√©dia, une **suite de tests compl√®te** est incluse.  
Elle couvre plusieurs aspects critiques :
* **Tests d'int√©grit√©** (`test_integrity.py`) üëâ V√©rifient que le spider extrait tous les champs attendus avec les bons types de donn√©es (titre, paragraphes, images, liens).
* **Tests de structure** (`test_structure.py`) üëâ Effectuent un crawl en direct sur Wikip√©dia pour d√©tecter si la structure HTML a chang√© (s√©lecteurs cass√©s).
* **Tests de cas limites** (`test_edge_cases.py`) üëâ Simulent des pages probl√©matiques (titre manquant, contenu vide, etc.).
* **Tests de navigation** (`test_navigation.py`) üëâ Valident que les r√®gles de filtrage des liens fonctionnent correctement.
* **Tests end-to-end** (`test_e2e.py`) üëâ Lancent le spider en tant que sous-processus et v√©rifient la g√©n√©ration du fichier JSON.
```sh
python run_tests.py
```
## **Tests et Qualit√© du Code**
### **Lancer les tests**
```bash
# Tests unitaires avec couverture
pytest --cov=. --cov-report=term-missing

# Tous les tests (incluant les tests live)
python run_tests.py --live
```
### **Couverture actuelle**
* **86%** de couverture globale
* **22 tests** r√©ussis (100% de r√©ussite)
* Tests unitaires pour tous les composants Scrapy
* Tests d'interface utilisateur (Streamlit) avec mocking complet
### **Analyse statique**
```bash
# V√©rification du style de code
ruff check .

# V√©rification des types
mypy .
```

## **R√©f√©rences et documentation**
[Scrapy ‚Äî Spiders](https://docs.scrapy.org/en/latest/topics/spiders.html)  
[Scrapy ‚Äî S√©lecteurs XPath](https://docs.scrapy.org/en/latest/topics/selectors.html)  
[Scrapy ‚Äî Param√®tres de pipeline et exports](https://docs.scrapy.org/en/latest/topics/feed-exports.html)  
[Documentation de Streamlit](https://docs.streamlit.io/)  
[Licences et conditions d‚Äôutilisation de Wikip√©dia](https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Droit_d%27auteur)  
[R√©capitulatif du RGPD](https://www.cnil.fr/fr/rgpd-par-ou-commencer)
___
"**_FelisCrawler_**" est con√ßu pour illustrer l‚Äôint√©gration de technologies de scraping et d‚Äôinterface utilisateur dans un contexte p√©dagogique et gouvern√©.