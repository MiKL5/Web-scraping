import streamlit as     st
import pandas    as     pd
from   pathlib   import Path
import time
import json
import csv
import subprocess


# Configurer la page
st.set_page_config(
    page_title="FelisCrawler",
    page_icon="ğŸ±",
    layout="wide"
)

st.title("ğŸ± FelisCrawler ğŸˆ")
st.markdown("")

# Initialiser la session
if "scraping" not in st.session_state:
    st.session_state["scraping"]    = False
if "last_output" not in st.session_state:
    st.session_state["last_output"] = None

# Sidebar des paramÃ¨tres
st.sidebar.header("âš™ï¸ Je configure le scraping")

depth_limit = st.sidebar.slider(
    "Quelle est la profondeur de crawl ?",
    min_value= 1,
    max_value= 5,
    value    = 4,
    help     = "C'est le nombre de niveaux de liens Ã  suivre. Plus c'est Ã©levÃ©, plus Ã§a consomme de ressources."
)

download_delay = st.sidebar.number_input(
    "Combien de secondes entre les requÃªtes ?",
    min_value  =   .5,
    max_value  = 10.0,
    value      =  1.23,
    step       =   .1,
    help       = "Temps d'attente entre chaque requÃªte HTTP. Il est recommandÃ© d'Ãªtre â‰¥ 1 s pour respecter les serveurs."
)

concurrent_requests = st.sidebar.slider(
    "Combien de requÃªtes simultanÃ©es ?",
    min_value       =  1,
    max_value       = 16,
    value           =  4,
    help            = "Plus il y a de requÃªtes en parallÃ¨le, plus le serveur est chargÃ©."
)

output_file = st.sidebar.text_input(
    "Le nom du fichier (JSON) est",
    value   = "result.json",
    help    = "Nom du fichier JSON oÃ¹ sauvegarder les rÃ©sultats du scraping."
)

st.sidebar.markdown("___")
st.sidebar.markdown("### ğŸŒ± Les recommandations sont")
st.sidebar.markdown(
    "* La profondeur de **_2 Ã  4_**\n"
    "* Le dÃ©lai doit Ãªtre **_â‰¥ 1 s_**\n"
    "* Et **_â‰¤ 8_** requÃªtes simultanÃ©es\n"
)
st.sidebar.markdown("<br><br>", unsafe_allow_html=True)

# Bouton pour crawler
if st.sidebar.button("ğŸš€ Je scrape !", type="primary"):
    st.session_state["scraping"]   = True
    st.session_state["start_time"] = time.time()

    cmd = [
        "scrapy" , "runspider" , "wikichat_spider.py",
        "-O", output_file,
        "-s", f"DEPTH_LIMIT={depth_limit}",
        "-s", f"DOWNLOAD_DELAY={download_delay}",
        "-s", f"CONCURRENT_REQUESTS={concurrent_requests}",
    ]

    st.sidebar.markdown("---")
    st.sidebar.markdown("### â±ï¸ Progression...")

    pages_metric = st.sidebar.empty()
    pages_metric.metric("Pages scrapÃ©es", 0)

    with st.spinner("ğŸ”„ Patience... je scrape..."):
        progress_bar   = st.progress(0)
        status_text    = st.empty()

        pages_count    = 0
        progress       = 0

        try:
            process     = subprocess.Popen(
                cmd,
                stdout  = subprocess.PIPE,
                stderr  = subprocess.STDOUT,
                text    = True,
                bufsize = 1
            )

            log_output  = []

            for line in process.stdout:
                line = line.strip()
                if not line:
                    continue

                log_output.append(line)
                status_text.text(line)

                # Heuristique simple : compter les pages crawlÃ©es
                if "Crawled" in line or "Scraped" in line:
                    pages_count += 1
                    pages_metric.metric("Pages scrapÃ©es", pages_count)

                # Progression pseudo-approximative (0 â†’ 90 %)
                progress = min(progress + 1, 90)
                progress_bar.progress(progress)

            process.wait()

            if process.returncode == 0:
                progress_bar.progress(100)
                st.success("âœ… Le scraping est terminÃ©.")
                st.session_state["scraping"]    = False
                st.session_state["last_output"] = output_file
                time.sleep(0.5)
                st.rerun()
            else:
                st.error(f"âŒ Erreur lors du scraping (code: {process.returncode})")
                with st.expander("ğŸ“‹ Voir les journaux"):
                    st.code("\n".join(log_output))

        except FileNotFoundError:
            st.error("âŒ Le spider est introuvable.")
        except Exception as e:
            st.error(f"âŒ Il y a une erreur ğŸ‘‰ {str(e)}")

# Charger les donnÃ©es
output_path = Path(output_file)
data        = None
df          = None

if output_path.exists():
    try:
        raw_text = output_path.read_text(encoding="utf-8")
        data     = json.loads(raw_text)

        if data:
            df_data = []
            for item in data:
                df_data.append({
                    "Titre"      : item.get("titre", "N/A"),
                    "Profondeur" : item.get("profondeur", 0),
                    "Paragraphes": item.get("nombre_paragraphes", 0),
                    "Images"     : item.get("nombre_images", 0),
                    "Longueur"   : item.get("longueur_contenu", 0),
                    "L'adresse"  : item.get("url", ""),
                })

            df = pd.DataFrame(df_data)

    except json.JSONDecodeError:
        st.error("âŒ Le fichier de rÃ©sultats est vide ou invalide. Lance un scraping.")
    except Exception as e:
        st.error(f"âŒ Erreur Ã  la lecture du fichier ğŸ‘‰ {str(e)}")

# Les onglets principaux
tab_config, tab_results, tab_plots, tab_ethics = st.tabs(
    ["âš™ï¸ La config. et l'Ã©tat", "ğŸ“Š Les rÃ©sultats", "ğŸ“ˆ Les graphiques", "âš–ï¸ Ã‰thique & infos"]
)

# 1er Onglet : La configuration
with tab_config:
    st.subheader("âš™ï¸ La configuration actuelle est")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(f"Profondeur", depth_limit)
    with col2:
        st.metric(f"DÃ©lai", download_delay)
    with col3:
        st.metric(f"RequÃªtes simultanÃ©es", concurrent_requests)
    with col4:
        st.write("**Le fichier de sortie est**")
        st.code(str(output_file))

    if st.session_state.get("scraping", False):
        st.info("ğŸŸ  Patience, je scrape... Consulte la sidebar pour suivre la progression.")
    elif df is not None:
        st.success(f"âœ… DonnÃ©es chargÃ©es depuis ğŸ‘‰ `{output_file}`")
    else:
        st.info("# â„¹ï¸ Il n'y a pas de rÃ©sultat. Lance le scraper.")

    if st.session_state.get("last_output"):
        st.markdown(
            f"ğŸ—ƒï¸ Le dernier fichier utilisÃ© est **`{st.session_state['last_output']}`**"
        )


# 2d onglet Les rÃ©sultats
with tab_results:
    st.subheader("ğŸ“Š Voici les rÃ©sultats du scraping")

    if data and df is not None and not df.empty:
        st.success(f"âœ… {len(df)} pages trouvÃ©es")

        # Statistiques globales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Pages", len(df))
        with col2:
            total_paragraphes = int(df["Paragraphes"].sum())
            st.metric("Paragraphes", total_paragraphes)
        with col3:
            total_images = int(df["Images"].sum())
            st.metric("Images", total_images)
        with col4:
            total_liens = sum(len(item.get("liens_internes", [])) for item in data)
            st.metric("Liens internes", total_liens)

        st.markdown("## **ğŸ” Filtrer les pages**")

        colf1, colf2, colf3, colf4 = st.columns(4)

        with colf1:
            depth_filter = st.multiselect(
                "Profondeur",
                options=sorted(df["Profondeur"].unique()),
                default=sorted(df["Profondeur"].unique()),
            )

        with colf2:
            search_term = st.text_input("Recherche dans les titres")

        with colf3:
            min_len = st.number_input(
                "Longueur minimale du contenu",
                min_value=0,
                max_value=int(df["Longueur"].max()),
                value=0,
                step=100,
            )

        with colf4:
            min_imgs = st.number_input(
                "Nombre minimal d'images",
                min_value=0,
                max_value=int(df["Images"].max()),
                value=0,
                step=1,
            )

        # Application des filtres
        filtered_df = df[df["Profondeur"].isin(depth_filter)]
        filtered_df = filtered_df[filtered_df["Longueur"] >= min_len]
        filtered_df = filtered_df[filtered_df["Images"] >= min_imgs]

        if search_term:
            filtered_df = filtered_df[
                filtered_df["Titre"].str.contains(search_term, case=False, na=False)
            ]

        st.markdown("### ğŸ“„ **Tableau des pages scrapÃ©es**")
        st.dataframe(
            filtered_df,
            use_container_width=True,
            hide_index=True,
        )

        st.markdown("### ğŸ” **Les dÃ©tails de page**")

        titles = [item.get("titre") for item in data if item.get("titre")]
        selected_title = st.selectbox(
            "Choisis une page", options=titles
        )

        if selected_title:
            selected_item = next(
                (item for item in data if item.get("titre") == selected_title),
                None,
            )

            if selected_item:
                cold1, cold2 = st.columns([2, 1])

                with cold1:
                    st.markdown(
                        f"L'adresse est [{selected_item['url']}]({selected_item['url']})"
                    )

                    if selected_item.get("introduction"):
                        st.markdown("L'introduction est ")
                        st.write(selected_item["introduction"])

                    # Liens internes
                    if selected_item.get("liens_internes"):
                        st.markdown("Les liens internes (â‰¤ 10) sont ")
                        for link in selected_item["liens_internes"][:10]:
                            st.markdown(f"- {link}")

                with cold2:
                    st.metric("La profondeur est ", selected_item.get("profondeur", 0))
                    st.metric(
                        "Le nombre de paragraphes est ",
                        selected_item.get("nombre_paragraphes", 0),
                    )
                    st.metric(
                        "Images",
                        selected_item.get("nombre_images", 0),
                    )

                    if selected_item.get("images"):
                        st.markdown("AperÃ§u jusqu'Ã  3 images")
                        for img in selected_item["images"][:3]:
                            try:
                                st.image(f"https:{img}", width=150)
                            except Exception:
                                st.text(img)

        st.markdown("## ğŸ’¾ Exporter les donnÃ©es")
        coldl1, coldl2 = st.columns(2)

        with coldl1:
            st.download_button(
                label="ğŸ“¥ RÃ©cupÃ©rer le fichier",
                data=json.dumps(data, indent=2, ensure_ascii=False),
                file_name=f"scraping_{time.strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
            )

    else:
        st.info("# â„¹ï¸ Il n'y a pas de rÃ©sultat. Tu peux lancer le scraper.")

# 3Ã¨me onglet : Les graphiques
with tab_plots:
    st.subheader("ğŸ“ˆ Les pages scrapÃ©es")

    if df is not None and not df.empty:
        colg1, colg2 = st.columns(2)

        with colg1:
            st.markdown("La rÃ©partition des pages par profondeur")
            depth_counts = df.groupby("Profondeur")["Titre"].count()
            st.bar_chart(depth_counts)

        with colg2:
            st.markdown("La relation longueur du contenu / nombre d'images")
            st.scatter_chart(df, x="Longueur", y="Images")

        st.markdown("La distribution du nombre de paragraphes")
        paragraphs_counts = df["Paragraphes"].value_counts().sort_index()
        st.bar_chart(paragraphs_counts)

        st.markdown(
            "Ces graphiques permettent de repÃ©rer rapidement les pages longues, riches en images ou particuliÃ¨rement denses en texte."
        )
    else:
        st.info("# â„¹ï¸ N'ayant pas de rÃ©sultat, il n'y a pas de graphique.")

# 4Ã¨me onglet L'Ã©thique & infos
with tab_ethics:
    st.subheader("âš–ï¸ Ã‰thique, droit, environnement et bonnes pratiques")

    st.markdown("### ğŸ“š Le cadre de WikipÃ©dia")
    st.markdown(
        "* Wikipedia impose des **conditions dâ€™utilisation** et une **licence CC BY-SA**.\n"
        "* Ce projet est conÃ§u pour un usage **pÃ©dagogique** et expÃ©rimental.\n"
        "* Toute rÃ©utilisation publique du contenu doit citer WikipÃ©dia et respecter la licence."
    )

    st.markdown("### ğŸ¤– Le scraping responsable")
    st.markdown(
        "* `ROBOTSTXT_OBEY=True` est activÃ© dans le spider Scrapy.\n"
        "* Le dÃ©lai entre requÃªtes (`DOWNLOAD_DELAY`) limite la charge envoyÃ©e au serveur.\n"
        "* Le nombre de requÃªtes simultanÃ©es (`CONCURRENT_REQUESTS`) doit rester raisonnable.\n"
        "* Ã‰vite dâ€™augmenter profondeur + requÃªtes simultanÃ©es + dÃ©lai trÃ¨s faible en mÃªme temps."
    )

    st.markdown("### ğŸ” Les donnÃ©es personnelles (RGPD)")
    st.markdown(
        "* Les pages WikipÃ©dia scrapÃ©es ici ne visent pas des **donnÃ©es Ã  caractÃ¨re personnel**.\n"
        "* Ce projet ne traite donc pas, en lâ€™Ã©tat, de donnÃ©es couvertes par le **RGPD**.\n"
        "* Si tu adaptes ce code pour dâ€™autres sites / contenus, tu devras vÃ©rifier :\n"
        "  * la prÃ©sence de donnÃ©es personnelles,\n"
        "  * la base lÃ©gale de traitement,\n"
        "  * les droits des personnes (accÃ¨s, suppression, etc.)."
    )

    st.markdown("### ğŸŒ L'impact environnemental")
    st.markdown(
        "* Chaque requÃªte HTTP consomme des ressources cÃ´tÃ© client et serveur.\n"
        "* Des paramÃ¨tres agressifs (profondeur Ã©levÃ©e, nombreuses requÃªtes simultanÃ©es, dÃ©lai faible) "
        "augmentent lâ€™empreinte carbone.\n"
        "* Utilise ce scraper avec **modÃ©ration** et prÃ©fÃ¨re des expÃ©riences limitÃ©es dans le temps et en volume."
    )

    st.markdown("### ğŸ§  Le management et les bonnes pratiques")
    st.markdown(
        "* DÃ©finis des **politiques internes** pour le scraping dans un contexte pro (lignes directrices, limites, logs).\n"
        "* Documente :\n"
        "  * les paramÃ¨tres utilisÃ©s,\n"
        "  * les cibles scrapÃ©es,\n"
        "  * les finalitÃ©s (pourquoi on scrape),\n"
        "  * les risques et mesures de mitigation.\n"
        "* Ce type dâ€™interface peut servir de **tableau de bord de gouvernance** du scraping."
    )

    st.markdown("___")
    st.markdown(
        "ğŸ± **FelisCrawler** | Scrapy et Streamlit â€” C'est un projet pÃ©dagogique. Il n'est pas destinÃ© Ã  une exploitation massive."
    )