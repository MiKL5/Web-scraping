# XHR Spider - Scraper de citations par Le filtrage dynamique<a href="../../"><img align="right" src="../../assets/atomicWebScraping.png" alt="Web scraping" height="64px"></a>
<div align="center">

![Python](https://img.shields.io/badge/Python-3.13.9-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scrapy](https://img.shields.io/badge/Scrapy-2.13.4-60A839?style=for-the-badge&logo=scrapy&logoColor=white)
![XPath](https://img.shields.io/badge/XPath-2.0-orange?style=for-the-badge)
![ASP.NET](https://img.shields.io/badge/ASP.NET-VIEWSTATE-512BD4?style=for-the-badge&logo=dotnet&logoColor=white)

</div><hr>

L'API JavaScript "`XMLHttpRequest`" est int√©gr√©e aux navigateurs permettant d‚Äôenvoyer des requ√™tes HTTP ou HTTPS de mani√®re asynchrone ; sans recharger la page enti√®re.  
Le but est d'extraire des citations filtr√©es par auteur et tag depuis quotes.toscrape.com avec gestion du `VIEWSTATE` "ASP.NET" et pagination automatique.
---
## **Les fonctionnalit√©s**
* **Filtrage dynamique** par auteur et tag via param√®tres CLI
* **Gestion du '`VIEWSTATE`'** (protection CSRF ASP.NET)
* **Pagination automatique** pour scraper tous les r√©sultats
* **Logs enrichis** avec des informations d√©taill√©es
* **Gestion d'erreurs** robuste avec '`errback'`
* **Nettoyage automatique** des guillemets
* **Param√®tres configurables** sans modifier le code
* **Export multi-formats** (`JSON`, `CSV`, `XML`, `JSONL`)
* **Async/await support** (Scrapy 2.13+)
## **Les technologies**
### Le stack principal
Technologie | Version | R√¥le | Badge
---|---|---|---
**Python** | 3.13.9 | Langage de programmation | ![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)
**Scrapy** | 2.13.4 | Framework de web scraping | ![Scrapy](https://img.shields.io/badge/Scrapy-60A839?logo=scrapy&logoColor=white)
**lxml** | 6.0.2 | Parser HTML/XML | ![lxml](https://img.shields.io/badge/lxml-orange)
**Twisted** | 25.5.0 | Framework r√©seau asynchrone | ![Twisted](https://img.shields.io/badge/Twisted-purple)
### Les protocoles & standards
Standard | Usage |
---|---
![XPath](https://img.shields.io/badge/XPath-2.0-orange) | S√©lection d'√©l√©ments DOM
![HTTP](https://img.shields.io/badge/HTTP-1.1%2F2.0-blue) | Protocole de communication
![POST](https://img.shields.io/badge/Method-POST-red) | Soumission de formulaires
![ASP.NET](https://img.shields.io/badge/ASP.NET-VIEWSTATE-512BD4?logo=dotnet) | Gestion d'√©tat c√¥t√© serveur
## **Les pr√©requis**
* **Python** >= 3.8
* **pip** >= 21.0
* **Git** (optionnel)
## üì¶ Installation
```bash
pip install -r requirements.txt
```
## **L'utiliser**
```bash
scrapy crawl xhr_spider.py -O data.json
```
## **L'architecture**
### Le flux de fonctionnement
```mermaid
graph TD
    A[start] --> B[GET /search.aspx]
    B --> C{VIEWSTATE trouv√©?}
    C -->|Non| D[Log erreur + Stop]
    C -->|Oui| E[POST /filter.aspx avec param√®tres]
    E --> F[Parse r√©ponse]
    F --> G{Citations trouv√©es?}
    G -->|Non| H[Log warning + Stop]
    G -->|Oui| I[Extraire donn√©es]
    I --> J{Page suivante existe?}
    J -->|Oui| E
    J -->|Non| K[Fin]
```
### **Le diagramme de s√©quence**
```mermaid
sequenceDiagram
    participant Spider
    participant Website
    participant Data
    
    Note over Spider: start() appel√©
    Spider->>Website: GET /search.aspx
    activate Website
    Website-->>Spider: HTML + Formulaire + VIEWSTATE
    deactivate Website
    
    Note over Spider: filter() - Extraction VIEWSTATE
    
    Spider->>Website: POST /filter.aspx<br/>(author, tag, VIEWSTATE)
    activate Website
    Note over Website: Validation formulaire<br/>Filtrage des citations
    Website-->>Spider: 302 Redirect ou HTML citations
    deactivate Website
    
    Note over Spider: parse() - Extraction donn√©es
    
    loop Pour chaque citation
        Spider->>Spider: XPath extraction<br/>(text, author, tags)
        Spider->>Spider: Nettoyage guillemets
        Spider->>Data: yield item
        activate Data
        Data-->>Spider: Item enregistr√©
        deactivate Data
    end
    
    alt Page suivante existe
        Spider->>Website: GET /page/2
        activate Website
        Website-->>Spider: HTML page suivante
        deactivate Website
        Note over Spider: Retour √† parse()
    else Derni√®re page
        Note over Spider: ‚úÖ Scraping termin√©
    end
    
    Note over Data: Export final<br/>(JSON/CSV/XML)
```
### **La structure du projet**
```python
class XhrSpiderSpider(scrapy.Spider):
    ‚îÇ
    ‚îú‚îÄ‚îÄ __init__()          # Initialisation avec param√®tres CLI
    ‚îÇ
    ‚îú‚îÄ‚îÄ start()             # Point d'entr√©e (async)
    ‚îÇ   ‚îî‚îÄ‚îÄ GET /search.aspx
    ‚îÇ
    ‚îú‚îÄ‚îÄ filter()            # Extraction VIEWSTATE + POST formulaire
    ‚îÇ   ‚îú‚îÄ‚îÄ Extraire VIEWSTATE
    ‚îÇ   ‚îî‚îÄ‚îÄ FormRequest avec formdata
    ‚îÇ
    ‚îú‚îÄ‚îÄ parse()             # Parser les r√©sultats
    ‚îÇ   ‚îú‚îÄ‚îÄ Extraire citations
    ‚îÇ   ‚îú‚îÄ‚îÄ Nettoyer donn√©es
    ‚îÇ   ‚îî‚îÄ‚îÄ Suivre pagination
    ‚îÇ
    ‚îî‚îÄ‚îÄ handle_error()      # Gestion des erreurs r√©seau
```
## **La configuration**
### **Les param√®tres du spider**
Param√®tre | Type | D√©faut | Description
---|---|---|---
`author` | str | "J.K. Rowling" | Auteur √† rechercher
`tag` | str | "dumbledore" | Tag √† filtrer
### **Les settings personnalis√©s**
```python
custom_settings = {
    'DOWNLOAD_DELAY'                : 1,              # D√©lai entre requ√™tes (secondes)
    'CONCURRENT_REQUESTS_PER_DOMAIN': 1,              # Nombre de requ√™tes parall√®les
    'ROBOTSTXT_OBEY'                : True,           # Respecter robots.txt
    'FEED_EXPORT_ENCODING'          : 'utf-8'         # Encodage des exports
}
```
### Modifier les settings globaux
√âditez `settings.py` :
```python
# Rate limiting
DOWNLOAD_DELAY           =  2  # Plus lent et plus s√ªr
AUTOTHROTTLE_ENABLED     = True
AUTOTHROTTLE_START_DELAY =  1
AUTOTHROTTLE_MAX_DELAY   = 10

# User-Agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'

# Logs
LOG_LEVEL = 'INFO'  # DEBUG, INFO, WARNING, ERROR
```
## **D√©pannage**
### **Le probl√®me 1 : Une `SyntaxError` avec les guillemets**
**Erreur :**
```sh
SyntaxError: unterminated string literal (detected at line 84)
text = text.strip('"'')
```
**Solution :**
```py
# ‚ùå Incorrect
text = text.strip('"'')

# ‚úÖ Correct
text = text.strip('"')
```
### **Le probl√®me 2 : VIEWSTATE non trouv√©**
**Logs :**
```
[xhr_spider] ERROR: ‚ùå VIEWSTATE n'est pas trouv√©
```
**Causes possibles :**
1. URL incorrecte (v√©rifier `/search.aspx`)
2. Structure HTML chang√©e
3. Site bloque le scraping
**Debug :**
```py
def filter(self, response):
    # Afficher le HTML brut
    self.logger.debug(response.text[:1000])
    
    # Tester diff√©rents s√©lecteurs
    viewstate = response.xpath("//input[@name='__VIEWSTATE']/@value").get()
    if not viewstate:
        viewstate = response.css("input[name='__VIEWSTATE']::attr(value)").get()
```
### **Le probl√®me 3 : Aucune citation trouv√©e**
**Logs :**
```
[xhr_spider] WARNING: ‚ö†Ô∏è Aucune citation trouv√©e pour Einstein / science
```
**Solutions :**
1. V√©rifier l'orthographe de l'auteur
2. V√©rifier que le tag existe
3. Inspecter la r√©ponse HTML
**Debug :**
```py
def parse(self, response):
    # Sauvegarder la page pour inspection
    with open('debug.html', 'w', encoding='utf-8') as f:
        f.write(response.text)
```
### **Le probl√®me 4 : Bloqu√© par le serveur (403/429)**
**Solutions :**
**1. Augmenter le d√©lai :**
```sh
scrapy runspider xhr_spider.py -s DOWNLOAD_DELAY=3
```
**2. Changer le User-Agent :**
```py
custom_settings = {
    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
```
**3. Activer AutoThrottle :**
```py
custom_settings = {
    'AUTOTHROTTLE_ENABLED'    : True,
    'AUTOTHROTTLE_START_DELAY':  2,
    'AUTOTHROTTLE_MAX_DELAY'  : 10
}
```
## **Les performances**
### **Les m√©triques typiques**
M√©trique | Valeur
---|---
Pages/minute | ~60 (avec DOWNLOAD_DELAY=1)
Citations/minute | ~300-600
Temps moyen/page | 1-2 secondes
Taux de succ√®s | >95%
### **L'optimisation**
**Pour des volumes importants :**
```py
custom_settings = {
    'CONCURRENT_REQUESTS'           : 16,
    'CONCURRENT_REQUESTS_PER_DOMAIN':  4,
    'DOWNLOAD_DELAY'                : .5,
    'AUTOTHROTTLE_ENABLED'          : True
}
```