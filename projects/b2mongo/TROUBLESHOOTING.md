# **üîß Troubleshooting - Books to MongoDB**
Guide de r√©solution des probl√®mes courants et solutions aux erreurs fr√©quentes.
## üóÑÔ∏è Erreurs MongoDB
### Erreur : "Connection refused" ou "Server not available"
**Sympt√¥mes** :
```
pymongo.errors.ServerSelectionTimeoutError: localhost:27017: 
[Errno 111] Connection refused
```
**Cause** : MongoDB n'est pas d√©marr√©.  
**Solutions** :
```sh
# Linux
sudo systemctl status mongod
sudo systemctl start mongod
sudo systemctl enable mongod  # D√©marrage automatique

# macOS
brew services start mongodb-community

# Windows
net start MongoDB

# V√©rifier le port
sudo netstat -tulpn | grep 27017
```
**Alternative** : Changer le port dans `settings.py` :
```py
MONGO_URI = 'mongodb://localhost:27018/'  # Port alternatif
```
### Erreur : "Authentication failed"
**Sympt√¥mes** :
```
pymongo.errors.OperationFailure: Authentication failed
```
**Cause** : MongoDB configur√© avec authentification mais identifiants incorrects.  
**Solutions** :
```py
# Dans settings.py
MONGO_URI = 'mongodb://username:password@localhost:27017/'
MONGO_DATABASE = 'books_toscrape'

# Ou cr√©er un utilisateur
```
```js
// Dans mongosh
use admin
db.createUser({
  user: "scraper",
  pwd: "password123",
  roles: [{role: "readWrite", db: "books_toscrape"}]
})
```
### Erreur : "Duplicate key error"
**Sympt√¥mes** :
```
pymongo.errors.DuplicateKeyError: E11000 duplicate key error collection: 
books_toscrape.books index: upc_1 dup key: { upc: "abc123" }
```
**Cause** : Tentative d'insertion d'un livre avec un UPC existant (ne devrait pas arriver avec upsert).  
**Solutions** :
```py
# V√©rifier que le pipeline utilise bien upsert=True
db.books.update_one(
    {'upc': item['upc']},
    {'$set': item_dict},
    upsert=True  # ‚Üê Important
)

# Si persistant, supprimer l'index et le recr√©er
```
```js
// Dans mongosh
db.books.dropIndex("upc_1")
db.books.createIndex({upc: 1}, {unique: true})
```
### Erreur : "Database locked"
**Sympt√¥mes** :
```
pymongo.errors.OperationFailure: database is locked
```
**Cause** : Arr√™t incorrect de MongoDB ou corruption.  
**Solutions** :
```sh
# Supprimer le fichier de lock
sudo rm /var/lib/mongodb/mongod.lock

# R√©parer la base
mongod --repair

# Red√©marrer
sudo systemctl start mongod
```
## üï∑Ô∏è Erreurs Scrapy
### Erreur : "ModuleNotFoundError: No module named 'scrapy'"
**Sympt√¥mes** :
```
ModuleNotFoundError: No module named 'scrapy'
```
**Cause** : Scrapy non install√© ou mauvais environnement virtuel.  
**Solutions** :
```sh
# V√©rifier l'environnement
which python
pip list | grep -i scrapy

# Installer Scrapy
pip install scrapy

# Ou r√©installer toutes les d√©pendances
pip install -r requirements.txt
```
### Erreur : "ImportError: cannot import name 'BookItem'"
**Sympt√¥mes** :
```
ImportError: cannot import name 'BookItem' from 'b2mongo.items'
```
**Cause** : Probl√®me de structure de projet ou fichier manquant.  
**Solutions** :
```sh
# V√©rifier la structure
ls -la b2mongo/
# Doit contenir : __init__.py, items.py, spiders/, etc.

# V√©rifier que __init__.py existe
touch b2mongo/__init__.py
touch b2mongo/spiders/__init__.py

# R√©installer le projet en mode d√©veloppement
pip install -e .
```
### Erreur : "Spider not found"
**Sympt√¥mes** :
```sh
$ scrapy crawl mongo
KeyError: 'Spider not found: mongo'
```
**Cause** : Mauvais dossier de travail ou nom incorrect.  
**Solutions** :
```sh
# V√©rifier que vous √™tes dans le bon dossier
pwd  # Doit afficher .../b2mongo

# Lister les spiders disponibles
scrapy list

# V√©rifier le nom du spider dans mongo.py
grep "name =" b2mongo/spiders/mongo.py
# Doit afficher : name = "mongo"

# Si le nom est diff√©rent
scrapy crawl <nom_affich√©>
```
### Erreur : "robots.txt disallow"
**Sympt√¥mes** :
```
[scrapy.downloadermiddlewares.robotstxt] DEBUG: 
Forbidden by robots.txt: <GET https://...>
```
**Cause** : Le site bloque le scraping via robots.txt.  
**Solutions** :
```py
# Dans settings.py
ROBOTSTXT_OBEY = False  # ‚ö†Ô∏è Utiliser avec pr√©caution

# books.toscrape.com autorise le scraping
# V√©rifier : https://books.toscrape.com/robots.txt
```
### Erreur : "Twisted reactor already installed"
**Sympt√¥mes** :
```
ReactorAlreadyInstalledError: reactor already installed
```
**Cause** : Conflit entre asyncio et twisted.  
**Solutions** :
```py
# Dans settings.py, commenter ou changer
# TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"

# Ou utiliser le reactor par d√©faut
TWISTED_REACTOR = "twisted.internet.selectreactor.SelectReactor"
```
## üñºÔ∏è Probl√®mes d'images
### Erreur : Images non t√©l√©charg√©es
**Sympt√¥mes** :
```
[scrapy.pipelines.images] WARNING: File (unknown-error): Error downloading file
```
**Causes possibles** :
1. URL d'image invalide
2. Pipeline d'images d√©sactiv√©
3. Dossier `images/` sans permissions  
**Solutions** :
```sh
# V√©rifier les permissions
ls -la images/
chmod -R 755 images/

# Cr√©er le dossier manuellement
mkdir -p images/full

# V√©rifier la configuration
grep IMAGES_STORE b2mongo/settings.py
# Doit afficher : IMAGES_STORE = 'images'

# V√©rifier que le pipeline est activ√©
grep BooksImagesPipeline b2mongo/settings.py
# Doit √™tre pr√©sent avec priority < 300
```
**Debug** :
```py
# Dans mongo.py, ajouter des logs
image_url = response.xpath('.../@src').get()
if image_url:
    absolute_url = response.urljoin(image_url)
    self.logger.info(f"üñºÔ∏è Image URL: {absolute_url}")  # ‚Üê Debug
    loader.add_value('image_url', absolute_url)
```
### Erreur : "OSError: [Errno 28] No space left on device"
**Sympt√¥mes** :
```
OSError: [Errno 28] No space left on device
```
**Cause** : Disque plein.  
**Solutions** :
```sh
# V√©rifier l'espace disque
df -h

# Nettoyer les anciennes images
rm -rf images/full/*

# Limiter le scraping
scrapy crawl mongo -a max_pages=5
```
### Images corrompues ou incompl√®tes
**Sympt√¥mes** : Images non ouvrables ou taille 0.  
**Cause** : T√©l√©chargement interrompu ou timeout.  
**Solutions** :
```py
# Dans settings.py, augmenter le timeout
DOWNLOAD_TIMEOUT = 60  # Au lieu de 30

# Augmenter les retries
RETRY_TIMES = 5

# Supprimer les images corrompues
```
```sh
# Trouver les images de taille 0
find images/full -type f -size 0 -delete

# Re-scraper pour t√©l√©charger √† nouveau
scrapy crawl mongo
```
## ‚ö° Probl√®mes de performance
### Scraping trop lent
**Sympt√¥mes** : Moins de 1 page/minute.  
**Causes** :
1. DOWNLOAD_DELAY trop √©lev√©
2. Concurrence trop faible
3. Probl√®me r√©seau  
**Solutions** :
```py
# Dans settings.py
DOWNLOAD_DELAY = 0.25  # R√©duire (attention au serveur)
CONCURRENT_REQUESTS_PER_DOMAIN = 4  # Augmenter
CONCURRENT_REQUESTS = 32  # Augmenter

# D√©sactiver les cookies si non n√©cessaires
COOKIES_ENABLED = False

# D√©sactiver certains logs
LOG_LEVEL = 'INFO'  # Au lieu de DEBUG
```
**Monitoring** :
```sh
# Voir la vitesse en temps r√©el
scrapy crawl mongo 2>&1 | grep "Crawled"

# R√©sultat typique :
# Crawled 150 pages (at 5 pages/min)
```
### MongoDB trop lent
**Sympt√¥mes** : Insertions lentes, CPU √©lev√©.  
**Causes** :
1. Pas d'index
2. Trop de connexions simultan√©es
3. MongoDB non optimis√©  
**Solutions** :
```js
// V√©rifier les index
db.books.getIndexes()

// Cr√©er les index si manquants
db.books.createIndex({upc: 1}, {unique: true})
db.books.createIndex({category: 1})
db.books.createIndex({rating: 1})

// Statistiques
db.books.stats()
```
```py
# Dans settings.py
MONGO_OPTIONS = {
    'maxPoolSize': 5,  # R√©duire le pool
    'socketTimeoutMS': 30000,
}
```
### M√©moire satur√©e
**Sympt√¥mes** : Process killed, OOM error.  
**Cause** : Trop d'items en m√©moire.  
**Solutions** :
```py
# Dans settings.py
# Limiter les requ√™tes concurrentes
CONCURRENT_REQUESTS = 16  # R√©duire

# Activer le garbage collector agressif
import gc
gc.set_threshold(700, 10, 10)

# Limiter la queue des items
CONCURRENT_ITEMS = 100
```
## üìä Probl√®mes de donn√©es
### Donn√©es manquantes ou None
**Sympt√¥mes** : Champs `None` dans MongoDB.  
**Causes** :
1. XPath incorrect
2. Structure HTML chang√©e
3. Processeur d√©faillant  
**Solutions** :
```sh
# Tester les XPath dans Scrapy shell
scrapy shell "https://books.toscrape.com/catalogue/..."

>>> response.xpath('//h1/text()').get()  # Test unitaire
>>> response.xpath('//table//tr[1]/td/text()').get()
```
**Debug dans le spider** :
```py
def parse_book(self, response):
    title = response.xpath('//h1/text()').get()
    self.logger.info(f"üìñ Titre extrait : {title}")  # ‚Üê Debug
    
    if not title:
        self.logger.error(f"‚ùå Titre manquant pour {response.url}")
```
### Prix incorrects (0.0 ou NaN)
**Sympt√¥mes** : Tous les prix √† 0.  
**Cause** : Processeur `clean_price()` d√©faillant.  
**Solutions** :
```py
# Tester le processeur
from b2mongo.items import clean_price

test_prices = [
    "¬£51.77",
    "√Ç¬£32.50",
    "√É‚Äö√Ç¬£10.00"
]

for p in test_prices:
    result = clean_price(p)
    print(f"{p} ‚Üí {result}")

# Si √©chec, am√©liorer la fonction
def clean_price(price):
    if price:
        # Supprimer tous les caract√®res non-num√©riques sauf . et ,
        import re
        cleaned = re.sub(r'[^\d.,]', '', price)
        cleaned = cleaned.replace(',', '.')
        try:
            return float(cleaned)
        except ValueError:
            return 0.0
    return 0.0
```
### Ratings invalides (0)
**Sympt√¥mes** : Tous les ratings √† 0.  
**Cause** : Extraction de la classe CSS √©choue.  
**Solutions** :
```py
# Dans parse_book()
rating_class = response.xpath('//p[contains(@class, "star-rating")]/@class').get()
self.logger.debug(f"‚≠ê Rating class: {rating_class}")  # ‚Üê Debug

if rating_class:
    rating = rating_class.split()[-1]
    self.logger.debug(f"‚≠ê Rating value: {rating}")  # ‚Üê Debug
    loader.add_value('rating', rating)
```
### Descriptions HTML non nettoy√©es
**Sympt√¥mes** : `<p>Text</p>` au lieu de `Text`.  
**Cause** : Processeur `remove_tags` non appliqu√©.  
**Solutions** :
```py
# V√©rifier items.py
description = scrapy.Field(
    input_processor=MapCompose(remove_tags, str.strip),  # ‚Üê Important
    output_processor=Join('\n')
)

# Tester manuellement
from w3lib.html import remove_tags
html = "<p>Test <strong>bold</strong></p>"
print(remove_tags(html))  # ‚Üí "Test bold"
```
## ‚öôÔ∏è Erreurs de configuration
### Erreur : "ITEM_PIPELINES not found"
**Sympt√¥mes** : Aucune donn√©e ins√©r√©e dans MongoDB.  
**Cause** : Pipelines non configur√©s.  
**Solutions** :
```py
# Dans settings.py, v√©rifier
ITEM_PIPELINES = {
    'b2mongo.pipelines.BooksImagesPipeline': 200,
    'b2mongo.pipelines.MongoDBPipeline': 300,
}

# Syntaxe correcte : chemin.complet.NomClasse: priorit√©
```
### Erreur : Settings non pris en compte
**Sympt√¥mes** : Modifications de `settings.py` ignor√©es.  
**Causes** :
1. Mauvais fichier `scrapy.cfg`
2. Settings surcharg√©s en ligne de commande  
**Solutions** :
```sh
# V√©rifier scrapy.cfg
cat scrapy.cfg
# [settings]
# default = b2mongo.settings  # ‚Üê Doit pointer vers settings.py

# Forcer les settings
scrapy crawl mongo -s DOWNLOAD_DELAY=1

# Voir les settings actifs
scrapy settings --get DOWNLOAD_DELAY
```
## üêõ Debugging
### Mode verbose
```sh
# Logs d√©taill√©s
scrapy crawl mongo -s LOG_LEVEL=DEBUG

# Encore plus de d√©tails
scrapy crawl mongo -s LOG_LEVEL=DEBUG -s DEPTH_STATS_VERBOSE=True
```
### Scrapy shell interactif
```sh
# Tester une page sp√©cifique
scrapy shell "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html"

# Dans le shell
>>> response.xpath('//h1/text()').get()
>>> view(response)  # Ouvre dans le navigateur
>>> fetch('https://autre-url.com')  # Changer d'URL
```
### Logs structur√©s
```py
# Ajouter des logs dans le spider
self.logger.debug(f"üêõ Debug: {variable}")
self.logger.info(f"‚ÑπÔ∏è Info: {message}")
self.logger.warning(f"‚ö†Ô∏è Warning: {issue}")
self.logger.error(f"‚ùå Error: {error}")
```
### Breakpoints Python
```py
# Dans le spider
def parse_book(self, response):
    import pdb; pdb.set_trace()  # ‚Üê Arr√™t debugger
    # Inspecter variables : print(response.url)
```
### Stats Scrapy
```py
# √Ä la fin du scraping
from scrapy.statscollectors import MemoryStatsCollector

stats = crawler.stats.get_stats()
print(f"Pages scrap√©es : {stats.get('response_received_count')}")
print(f"Items scrap√©s : {stats.get('item_scraped_count')}")
print(f"Erreurs : {stats.get('log_count/ERROR', 0)}")
```
## üìû Obtenir de l'aide
### Checklist avant de demander de l'aide
- [ ] MongoDB est-il d√©marr√© ? (`sudo systemctl status mongod`)
- [ ] Les d√©pendances sont-elles install√©es ? (`pip list`)
- [ ] √ätes-vous dans le bon dossier ? (`scrapy list`)
- [ ] Les logs contiennent-ils des erreurs ? (chercher `ERROR`)
- [ ] Avez-vous test√© en mode DEBUG ? (`-s LOG_LEVEL=DEBUG`)
### Informations √† fournir
```sh
# Version Python
python --version

# Version Scrapy
scrapy version -v

# Version MongoDB
mongod --version

# Structure du projet
tree -L 2 b2mongo/

# Logs complets
scrapy crawl mongo 2>&1 | tee error.log
```
### Ressources externes
- **Documentation Scrapy** : https://docs.scrapy.org/
- **Documentation MongoDB** : https://docs.mongodb.com/
- **Stack Overflow** : Tag `scrapy` ou `pymongo`
- **GitHub Issues** : Ouvrir un ticket avec les logs
## üéØ Checklist de diagnostic rapide
Probl√®me | Commande de diagnostic
---|---
MongoDB down | `sudo systemctl status mongod`
Port occup√© | `netstat -tulpn \| grep 27017`
D√©pendances manquantes | `pip list \| grep -E "scrapy\|pymongo"`
Spider introuvable | `scrapy list`
Permissions images | `ls -la images/`
Espace disque | `df -h`
Settings actifs | `scrapy settings --get ITEM_PIPELINES`
XPath incorrect | `scrapy shell <URL>`
___
**Conseil** : La plupart des probl√®mes se r√©solvent en activant les logs DEBUG et en lisant attentivement les messages d'erreur.