# ğŸ—ï¸ Architecture technique - Books to MongoDB
Documentation dÃ©taillÃ©e de l'architecture du projet, des composants et des flux de donnÃ©es.
## ğŸ¯ Vue d'ensemble
### Stack technique
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â”‚   Spider (mongo.py)     â”‚      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Processing Layer            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚  Items   â”‚  â”‚Processorsâ”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Pipeline Layer              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚  Images  â”‚  â”‚ MongoDB  â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Storage Layer               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚   FS     â”‚  â”‚ MongoDB  â”‚       â”‚
â”‚    â”‚ (images) â”‚  â”‚   DB     â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Technologies
* **Scrapy 2.11+** : Framework de scraping
* **PyMongo 4.0+** : Driver MongoDB
* **Pillow 10.0+** : Traitement d'images
* **Python 3.8+** : Langage
## ğŸ•¸ï¸ Architecture Scrapy
### Cycle de vie d'une requÃªte
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. start_urls                                      â”‚
â”‚     â””â”€â”€> parse()                                    â”‚
â”‚            â””â”€â”€> parse_category()                    â”‚
â”‚                   â””â”€â”€> parse_book()                 â”‚
â”‚                          â””â”€â”€> yield Item            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Item Processors                                 â”‚
â”‚     â”œâ”€â”€> clean_price()                              â”‚
â”‚     â”œâ”€â”€> clean_stock()                              â”‚
â”‚     â”œâ”€â”€> rating_to_number()                         â”‚
â”‚     â””â”€â”€> remove_tags()                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Pipelines (order: 200, 300)                     â”‚
â”‚     â”œâ”€â”€> BooksImagesPipeline (200)                  â”‚
â”‚     â”‚      â””â”€â”€> Download image                      â”‚
â”‚     â”‚           â””â”€â”€> Save to images/full/           â”‚
â”‚     â”‚                â””â”€â”€> Add image_path to item    â”‚
â”‚     â”‚                                               â”‚
â”‚     â””â”€â”€> MongoDBPipeline (300)                      â”‚
â”‚            â””â”€â”€> upsert to MongoDB                   â”‚
â”‚                 â””â”€â”€> Log statistics                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### PrioritÃ© des pipelines
L'ordre est crucial :
1. **BooksImagesPipeline (200)** : S'exÃ©cute en premier pour tÃ©lÃ©charger l'image
2. **MongoDBPipeline (300)** : S'exÃ©cute aprÃ¨s pour avoir `image_path` complet
## ğŸ§© Composants dÃ©taillÃ©s
### 1. Spider : `mongo.py`
**ResponsabilitÃ©s** :
* Extraction des URLs de catÃ©gories
* Navigation dans la pagination
* Extraction des dÃ©tails de chaque livre
* Application des filtres (catÃ©gorie, pages)
**MÃ©thodes principales** :
```py
parse(response)
â”œâ”€â”€ Extrait les catÃ©gories du menu
â”œâ”€â”€ Applique le filtre de catÃ©gorie
â””â”€â”€ yield response.follow() â†’ parse_category

parse_category(response)
â”œâ”€â”€ Extrait les livres de la page
â”œâ”€â”€ yield response.follow() â†’ parse_book
â””â”€â”€ GÃ¨re la pagination (next page)

parse_book(response)
â”œâ”€â”€ Utilise ItemLoader pour extraire les donnÃ©es
â”œâ”€â”€ Applique les processeurs automatiquement
â””â”€â”€ yield item â†’ Pipelines
```
**Gestion de la pagination** :
```py
# DÃ©tection automatique du bouton "Next"
next_page = response.xpath('//li[@class="next"]/a/@href').get()

# VÃ©rification de la limite
if self.max_pages and self.pages_scraped >= self.max_pages:
    return

# Suivi automatique
yield response.follow(next_page, callback=self.parse_category)
```
### 2. Items : `items.py`
**Structure BookItem** :
```mermaid
graph LR
    A[BookItem] --> B[Titre<br/>str]
    A --> C[URL<br/>str]
    A --> D[UPC<br/>str ğŸ”‘]
    A --> E[prix_excl_tax<br/>float âœ“]
    A --> F[prix_incl_tax<br/>float âœ“]
    A --> G[tax<br/>float âœ“]
    A --> H[availability<br/>int âœ“]
    A --> I[rating<br/>1-5 int âœ“]
    A --> J[category<br/>str]
    A --> K[description<br/>HTML nettoyÃ©]
    A --> L[number_of_reviews<br/>int]
    A --> M[image_url<br/>str]
    A --> N[image_path<br/>local âœ“]
    
    E -.->|clean_price<br/>Â£â†’float| O[MapCompose]
    H -.->|clean_stock<br/>regexâ†’int| O
    I -.->|rating_to_number<br/>Oneâ†’1| O
    K -.->|remove_tags+Join<br/>'\n'| O
    
    style D fill:#ffcdd2
    style O fill:#fff9c4
```
**Processeurs personnalisÃ©s** :
1. **clean_price()** :
   ```py
   "Â£51.77" â†’ 51.77 (float)
   ```
2. **clean_stock()** :
   ```py
   "In stock (22 available)" â†’ 22 (int)
   ```
3. **rating_to_number()** :
   ```py
   "Three" â†’ 3 (int)
   ```
4. **remove_tags() + Join()** :
   ```py
   "<p>Text</p><p>More</p>" â†’ "Text\nMore"
   ```
### 3. Pipelines : `pipelines.py`
**Architecture des pipelines** :
```mermaid
flowchart TD
    A[Scrapy Engine] --> B["Pipeline 200<br/>BooksImagesPipeline"]
    A --> C["Pipeline 300<br/>MongoDBPipeline"]
    
    B --> H["get_media_requests()<br/>â†’ Request(image_url)"]
    H --> I["TÃ©lÃ©chargement image"]
    I --> J["item_completed()<br/>â†’ image_path"]
    
    C --> D["Connexion MongoDB<br/>open_spider()"]
    D --> E["Index: upc(unique)<br/>category, rating"]
    C --> F["process_item()<br/>upsert par UPC"]
    F --> G["Fermeture+stats<br/>close_spider()"]
    
    style B fill:#e1f5fe
    style C fill:#c8e6c9
```
#### 3.1 MongoDBPipeline
**Cycle de vie** :
```py
open_spider()
â”œâ”€â”€ Connexion Ã  MongoDB
â”œâ”€â”€ CrÃ©ation des index
â”‚   â”œâ”€â”€ upc (unique)
â”‚   â”œâ”€â”€ category
â”‚   â””â”€â”€ rating
â””â”€â”€ Log de confirmation

process_item()
â”œâ”€â”€ Conversion item â†’ dict
â”œâ”€â”€ Upsert dans MongoDB
â”‚   â””â”€â”€ Update si upc existe
â”‚   â””â”€â”€ Insert si nouveau
â””â”€â”€ Return item (pour pipeline suivant)

close_spider()
â”œâ”€â”€ Statistiques finales
â””â”€â”€ Fermeture connexion
```
**StratÃ©gie d'upsert** :
```py
result = db.books.update_one(
    {'upc': item['upc']},      # CritÃ¨re de recherche
    {'$set': item_dict},        # DonnÃ©es Ã  mettre Ã  jour
    upsert=True                 # CrÃ©er si inexistant
)
```
**Avantages** :
- âœ… Pas de doublons (upc unique)
- âœ… Mises Ã  jour automatiques
- âœ… Idempotent (re-scraping safe)
#### 3.2 BooksImagesPipeline
**HÃ©ritage** : `ImagesPipeline` (Scrapy built-in)  
**Processus** :
```py
get_media_requests()
â”œâ”€â”€ Extrait image_url de l'item
â”œâ”€â”€ Validation URL (http/https)
â””â”€â”€ yield scrapy.Request(url)

# Scrapy tÃ©lÃ©charge automatiquement

item_completed()
â”œâ”€â”€ VÃ©rifie le succÃ¨s du tÃ©lÃ©chargement
â”œâ”€â”€ Ajoute image_path Ã  l'item
â””â”€â”€ Return item modifiÃ©
```
**Nommage automatique** :
```
image_url: https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg
          â†“ SHA1 hash
image_path: full/2cdad67c44b002e7ead0cc35693c0e8b.jpg
```
### 4. Configuration : `settings.py`
**Sections principales** :

```py
# 1. IdentitÃ© du bot
BOT_NAME = "b2mongo"
USER_AGENT = "Mozilla/5.0..."

# 2. Politesse
ROBOTSTXT_OBEY = True
DOWNLOAD_DELAY = 0.5
CONCURRENT_REQUESTS_PER_DOMAIN = 2

# 3. Connexion MongoDB
MONGO_URI = 'mongodb://localhost:27017/'
MONGO_DATABASE = 'books_toscrape'

# 4. Stockage images
IMAGES_STORE = 'images'

# 5. Retry & Timeout
RETRY_TIMES = 3
DOWNLOAD_TIMEOUT = 30

# 6. Pipelines (ordre d'exÃ©cution)
ITEM_PIPELINES = {
    'b2mongo.pipelines.BooksImagesPipeline': 200,
    'b2mongo.pipelines.MongoDBPipeline': 300,
}
```
## ğŸ”„ Flux de donnÃ©es
### Diagramme de flux complet
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START                                                      â”‚
â”‚    â”‚                                                        â”‚
â”‚    â–¼                                                        â”‚
â”‚  start_urls = ["https://books.toscrape.com/"]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  parse(response)                                            â”‚
â”‚    â”œâ”€ Extract categories (sidebar menu)                     â”‚
â”‚    â”œâ”€ Filter by category (if specified)                     â”‚
â”‚    â””â”€ For each category:                                    â”‚
â”‚         â””â”€ yield response.follow() â†’ parse_category         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  parse_category(response)                                   â”‚
â”‚    â”œâ”€ Extract book URLs (20 per page)                       â”‚
â”‚    â”œâ”€ For each book:                                        â”‚
â”‚    â”‚    â””â”€ yield response.follow() â†’ parse_book             â”‚
â”‚    â”‚                                                        â”‚
â”‚    â””â”€ Pagination:                                           â”‚
â”‚         â”œâ”€ Check max_pages limit                            â”‚
â”‚         â””â”€ If next page exists:                             â”‚
â”‚              â””â”€ yield response.follow() â†’ parse_category    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  parse_book(response)                                       â”‚
â”‚    â”œâ”€ ItemLoader extracts all fields:                       â”‚
â”‚    â”‚    â”œâ”€ title, url, upc, category                        â”‚
â”‚    â”‚    â”œâ”€ prices (with clean_price processor)              â”‚
â”‚    â”‚    â”œâ”€ availability (with clean_stock processor)        â”‚
â”‚    â”‚    â”œâ”€ rating (with rating_to_number processor)         â”‚
â”‚    â”‚    â”œâ”€ description (with remove_tags processor)         â”‚
â”‚    â”‚    â””â”€ image_url                                        â”‚
â”‚    â”‚                                                        â”‚
â”‚    â””â”€ yield loader.load_item()                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ITEM PROCESSING                                            â”‚
â”‚    â”œâ”€ Input processors execute:                             â”‚
â”‚    â”‚    â”œâ”€ clean_price: "Â£51.77" â†’ 51.77                    â”‚
â”‚    â”‚    â”œâ”€ clean_stock: "In stock (22)" â†’ 22                â”‚
â”‚    â”‚    â”œâ”€ rating_to_number: "Three" â†’ 3                    â”‚
â”‚    â”‚    â””â”€ remove_tags: "<p>Text</p>" â†’ "Text"              â”‚
â”‚    â”‚                                                        â”‚
â”‚    â””â”€ Output processors execute:                            â”‚
â”‚         â”œâ”€ TakeFirst(): list â†’ single value                 â”‚
â”‚         â””â”€ Join('\n'): list â†’ concatenated string           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BooksImagesPipeline (priority: 200)                        â”‚
â”‚    â”œâ”€ get_media_requests():                                 â”‚
â”‚    â”‚    â”œâ”€ Extract image_url                                â”‚
â”‚    â”‚    â”œâ”€ Validate URL                                     â”‚
â”‚    â”‚    â””â”€ yield scrapy.Request(image_url)                  â”‚
â”‚    â”‚                                                        â”‚
â”‚    â”œâ”€ [Scrapy downloads image]                              â”‚
â”‚    â”‚                                                        â”‚
â”‚    â””â”€ item_completed():                                     â”‚
â”‚         â”œâ”€ Check download success                           â”‚
â”‚         â”œâ”€ Add image_path to item                           â”‚
â”‚         â””â”€ Return modified item                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDBPipeline (priority: 300)                            â”‚
â”‚    â”œâ”€ Convert item to dict                                  â”‚
â”‚    â”œâ”€ Upsert to MongoDB:                                    â”‚
â”‚    â”‚    db.books.update_one(                                â”‚
â”‚    â”‚        {'upc': item['upc']},                           â”‚
â”‚    â”‚        {'$set': item_dict},                            â”‚
â”‚    â”‚        upsert=True                                     â”‚
â”‚    â”‚    )                                                   â”‚
â”‚    â””â”€ Log result (inserted/updated)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  END                                                        â”‚
â”‚    â””â”€ Item stored in MongoDB + Image saved to disk          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## ğŸ›ï¸ Configuration avancÃ©e
### Gestion de la concurrence
```py
# RequÃªtes simultanÃ©es par domaine
CONCURRENT_REQUESTS_PER_DOMAIN = 2

# RequÃªtes simultanÃ©es globales
CONCURRENT_REQUESTS = 16

# Pool de connexions MongoDB
MONGO_OPTIONS = {
    'maxPoolSize': 10,
}
```
### Gestion des erreurs
```py
# Retry automatique
RETRY_ENABLED = True
RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]

# Timeout
DOWNLOAD_TIMEOUT = 30

# Redirection
REDIRECT_ENABLED = True
REDIRECT_MAX_TIMES = 3
```
### Optimisations MongoDB
**Index crÃ©Ã©s automatiquement** :
```py
db.books.create_index('upc', unique=True)      # ClÃ© primaire
db.books.create_index('category')              # Filtrage rapide
db.books.create_index('rating')                # Tri par note
```
**Avantages** :
* Upsert O(log n) au lieu de O(n)
* RequÃªtes par catÃ©gorie optimisÃ©es
* Tri par rating instantanÃ©
## ğŸš€ Optimisations
### 1. PrÃ©chargement des index
```py
# Dans open_spider()
self.db.books.create_index('upc', unique=True, background=True)
self.db.books.create_index('category', background=True)
```
### 2. Batch inserts (future amÃ©lioration)
```py
# Au lieu d'insÃ©rer 1 par 1
items_buffer = []
if len(items_buffer) >= 100:
    db.books.insert_many(items_buffer)
    items_buffer.clear()
```
### 3. Cache DNS
```py
DNSCACHE_ENABLED = True
DNSCACHE_SIZE = 10000
```
### 4. HTTP compression
```py
COMPRESSION_ENABLED = True
```
## ğŸ“Š MÃ©triques et monitoring
### Logs structurÃ©s
```py
spider.logger.info(f"ğŸ“š {len(categories)} catÃ©gories trouvÃ©es")
spider.logger.info(f"ğŸ“• {len(books)} livres dans {category}")
spider.logger.debug(f"âœ… {title} ajoutÃ©")
```
### Statistiques Scrapy
```py
# Statistiques automatiques collectÃ©es
STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector'

# Accessibles via crawler.stats
stats = crawler.stats.get_stats()
# downloader/request_count
# downloader/response_count
# item_scraped_count
# etc.
```
### Monitoring MongoDB
```py
# Dans close_spider()
count = self.db.books.count_documents({})
by_category = self.db.books.aggregate([
    {'$group': {'_id': '$category', 'count': {'$sum': 1}}}
])
```
## ğŸ” SÃ©curitÃ©
### Validation des donnÃ©es
```py
# Validation URL d'image
if isinstance(image_url, str) and image_url.startswith('http'):
    yield scrapy.Request(image_url)
```
### Gestion des exceptions
```py
try:
    self.client = pymongo.MongoClient(self.mongo_uri)
    self.db = self.client[self.mongo_db]
except Exception as e:
    spider.logger.error(f"âŒ Erreur MongoDB: {e}")
    raise
```
### Protection contre les injections
```py
# PyMongo Ã©chappe automatiquement
# Pas besoin de sanitization manuelle
db.books.update_one({'upc': item['upc']}, {'$set': item_dict})
```
## ğŸ§ª Tests et debugging
### Mode debug
```sh
scrapy crawl mongo -s LOG_LEVEL=DEBUG
```
### Scrapy shell
```sh
# Tester les XPath interactivement
scrapy shell "https://books.toscrape.com/"

# Dans le shell
>>> response.xpath('//article[@class="product_pod"]').getall()
```
### VÃ©rification MongoDB
```javascript
// Statistiques
db.books.aggregate([
  {$group: {_id: "$category", count: {$sum: 1}}},
  {$sort: {count: -1}}
])

// IntÃ©gritÃ© des donnÃ©es
db.books.find({$or: [
  {title: {$exists: false}},
  {price_incl_tax: {$lte: 0}},
  {upc: {$exists: false}}
]})
```
## ğŸ“š RÃ©fÃ©rences techniques
* [Scrapy Documentation](https://docs.scrapy.org/)
* [MongoDB Manual](https://docs.mongodb.com/)
* [ItemLoader](https://docs.scrapy.org/en/latest/topics/loaders.html)
* [ImagesPipeline](https://docs.scrapy.org/en/latest/topics/media-pipeline.html)
___
Pour des dÃ©tails d'implÃ©mentation, consultez le code source commentÃ© dans chaque fichier.