# ‚öñÔ∏è √âthique et bonnes pratiques - Web Scraping
Guide d'√©thique et de bonnes pratiques pour un scraping responsable et l√©gal.
## **üéØ Les principes fondamentaux**
### 1. Le respect avant tout
Le web scraping n'est **pas** un droit acquis. C'est un privil√®ge qui s'accompagne de responsabilit√©s :
* ‚úÖ **Respecter** les propri√©taires des sites web
* ‚úÖ **Minimiser** l'impact sur les serveurs
* ‚úÖ **Se conformer** aux lois et r√©glementations
* ‚úÖ **Prot√©ger** les donn√©es personnelles
* ‚úÖ **√ätre transparent** sur vos intentions
### 2. La r√®gle d'or
> **"Scrapez les autres sites comme vous aimeriez que votre propre site soit scrap√©."**

Si une action pourrait nuire, ralentir ou co√ªter de l'argent au propri√©taire du site, ne la faites pas.
## ‚öñÔ∏è Aspects l√©gaux
### Cadre juridique g√©n√©ral
**‚ö†Ô∏è AVERTISSEMENT** : Ce projet est un **outil √©ducatif** et ne constitue pas un conseil juridique. Consultez toujours un avocat pour des questions l√©gales sp√©cifiques.
### 1. Conditions d'utilisation (ToS)
```
‚úÖ Toujours lire les Terms of Service du site
‚úÖ Respecter les interdictions explicites
‚ùå Ne jamais contourner les protections techniques
‚ùå Ne pas scraper si c'est explicitement interdit
```
**Example** :
```
Site A : "No automated scraping allowed" ‚Üí ‚ùå Ne pas scraper
Site B : Pas de mention ‚Üí ‚ö†Ô∏è Proc√©der avec prudence
Site C : "API available for data access" ‚Üí ‚úÖ Utiliser l'API
```
### 2. Robots.txt
**Le fichier `robots.txt` est une convention**, pas une loi, mais il exprime clairement les souhaits du propri√©taire.
```python
# Ce projet RESPECTE robots.txt par d√©faut
ROBOTSTXT_OBEY = True  # ‚Üê Ne JAMAIS changer en False sans raison valable
```
**V√©rifier robots.txt** :
```bash
curl https://example.com/robots.txt

# Exemple books.toscrape.com :
# User-agent: *
# Disallow:
# ‚Üí Pas de restriction, scraping autoris√©
```
### 3. L√©gislation par r√©gion
R√©gion | Loi applicable | Points cl√©s
---|---|---
**UE** | RGPD | Protection donn√©es personnelles
**USA** | CFAA, DMCA | Acc√®s non autoris√© interdit
**France** | LPD, Code p√©nal | Art. 323-1 (acc√®s frauduleux)
**UK** | Computer Misuse Act | Acc√®s non autoris√© puni

**R√®gle g√©n√©rale** : Le scraping de **donn√©es publiques** est g√©n√©ralement l√©gal, mais :
* ‚ùå **Jamais** de donn√©es personnelles sans consentement
* ‚ùå **Jamais** de contenu prot√©g√© par copyright
* ‚ùå **Jamais** de contournement d'authentification
### 4. Pr√©c√©dents juridiques notables
**Cas hiQ Labs vs LinkedIn (2022)** :
* ‚úÖ Scraping de donn√©es **publiques** : L√©gal
* ‚ùå Scraping apr√®s interdiction explicite : Risqu√©
**R√®gle pratique** :
```
Donn√©es publiques + robots.txt OK + pas de ToS viol√©s = Probablement OK
Donn√©es priv√©es OU authentification requise = NE PAS SCRAPER
```
## **üîß Respect technique**
### 1. Rate limiting (limitation de d√©bit)
**Pourquoi c'est important** :
* √âviter la surcharge des serveurs
* Ne pas impacter les utilisateurs l√©gitimes
* Ne pas √™tre banni
**Impl√©mentation dans ce projet** :
```python
# settings.py
DOWNLOAD_DELAY = 0.5                    # 500ms entre requ√™tes
CONCURRENT_REQUESTS_PER_DOMAIN = 2      # Max 2 requ√™tes simultan√©es
CONCURRENT_REQUESTS = 16                # Global limit√©

# ‚ùå MAUVAIS EXEMPLE (ne pas faire)
DOWNLOAD_DELAY = 0
CONCURRENT_REQUESTS_PER_DOMAIN = 50
```
**Calcul d'impact** :
```
Param√®tres actuels :
- 2 requ√™tes/seconde max par domaine
- 0.5s d√©lai entre requ√™tes

Impact : ~120 requ√™tes/minute
‚Üí Tr√®s raisonnable pour un site de test

‚ö†Ô∏è Sur un site de production :
- Augmenter DOWNLOAD_DELAY √† 1-2 secondes
- Scraper pendant les heures creuses
```
### 2. User-Agent honn√™te
```python
# BON : User-Agent identifiable
USER_AGENT = "Mozilla/5.0 (Educational Scraper; +http://myproject.com/bot)"

# ACCEPTABLE : User-Agent navigateur standard
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# MAUVAIS : Se faire passer pour un utilisateur l√©gitime syst√©matiquement
USER_AGENT = "GoogleBot/2.1"  # Usurpation d'identit√©
```
### 3. Caching et d√©duplication
```python
# Ne pas re-t√©l√©charger les m√™mes donn√©es
# MongoDB upsert √©vite les doublons
db.books.update_one(
    {'upc': item['upc']},
    {'$set': item_dict},
    upsert=True  # ‚Üê Mise √† jour au lieu de duplication
)

# Cache HTTP activ√©
HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 3600  # 1 heure
```
### 4. Gestion des erreurs
```python
# Retry intelligent (ne pas marteler le serveur)
RETRY_ENABLED = True
RETRY_TIMES = 3                        # Seulement 3 tentatives
RETRY_HTTP_CODES = [500, 502, 503]    # Seulement erreurs serveur

# Backoff exponentiel (temps entre retries augmente)
RETRY_BACKOFF = 2  # 1s, 2s, 4s, ...
```
## üéì Cas d'usage de ce projet
### books.toscrape.com : Un cas particulier
**Ce site est explicitement con√ßu pour l'apprentissage du scraping.**
```
‚úÖ Objectif : Formation et tests
‚úÖ robots.txt : Pas de restriction
‚úÖ ToS : Pas de conditions interdisant le scraping
‚úÖ Propri√©taire : Consent explicite au scraping √©ducatif
‚úÖ Donn√©es : Fictives (pas de vraies personnes/entreprises)
```
**C'est pourquoi ce projet est √©thique** :
1. Site de d√©monstration (pas de production)
2. Donn√©es fictives (pas de PII)
3. Impact minimal (serveur pr√©vu pour √ßa)
4. Objectif p√©dagogique clair
### ‚ö†Ô∏è NE PAS UTILISER CE CODE SUR :
Site type | Raison | Alternative
---|---|---
E-commerce r√©el | Impact business, ToS | Utiliser leur API
R√©seaux sociaux | RGPD, ToS strict | API officielle
Sites d'actualit√©s | Copyright | Flux RSS
Sites gouvernementaux | Donn√©es sensibles | Open Data portals
Sites prot√©g√©s par login | Violation ToS | Demander acc√®s API
## ‚úÖ Bonnes pratiques
### 1. Avant de commencer
```
‚ñ° Lire les ToS du site
‚ñ° V√©rifier robots.txt
‚ñ° Chercher une API officielle (pr√©f√©rable au scraping)
‚ñ° √âvaluer l'impact potentiel
‚ñ° Documenter l'objectif du scraping
‚ñ° Contacter le propri√©taire si doute (meilleure pratique)
```
### 2. Pendant le scraping
```python
# Identifier clairement votre bot
USER_AGENT = "MyResearchBot/1.0 (+mailto:contact@example.com)"

# Limiter la charge
DOWNLOAD_DELAY = 1  # Au moins 1 seconde

# Respecter les heures de faible trafic
# Scraper la nuit si possible

# Logger toutes les actions
LOG_LEVEL = 'INFO'
LOG_FILE = 'scraping.log'

# Monitorer l'impact
stats = crawler.stats.get_stats()
```
### 3. Apr√®s le scraping
```
‚ñ° Nettoyer les donn√©es (supprimer PII si pr√©sentes par erreur)
‚ñ° Ne pas publier de donn√©es sensibles
‚ñ° Respecter le copyright du contenu
‚ñ° Documenter la source des donn√©es
‚ñ° Mettre √† jour r√©guli√®rement (pas scraper en continu)
```
### 4. Stockage et utilisation des donn√©es
```python
# BON : Donn√©es anonymis√©es et agr√©g√©es
stats = {
    'avg_price_by_category': {...},
    'total_books': 1000,
    'top_rated_categories': [...]
}

# MAUVAIS : Publication de donn√©es compl√®tes
# Ne PAS publier dumps complets de bases de donn√©es scrap√©es
```
## **üö´ Que faire / Ne pas faire**
### ‚úÖ √Ä FAIRE
Action | Raison
---|---
Lire robots.txt | Respect des r√®gles
Utiliser des d√©lais | Ne pas surcharger
S'identifier clairement | Transparence
Utiliser l'API si disponible | M√©thode privil√©gi√©e
Scraper donn√©es publiques uniquement | L√©galit√©
Respecter le copyright | Loi
Monitorer l'impact | Responsabilit√© 
Cacher les requ√™tes | Efficacit√© 
### ‚ùå √Ä NE PAS FAIRE
Action | Raison
---|---
Ignorer robots.txt | Irrespect flagrant
Faire du scraping agressif | Surcharge serveur
Usurper un User-Agent | Malhonn√™tet√©
Scraper des donn√©es priv√©es | Ill√©gal (RGPD)
Contourner des protections | Potentiellement ill√©gal
Revendre les donn√©es | Copyright, √©thique
Scraper en continu 24/7 | Abus de ressources
Publier les donn√©es brutes | Respect propri√©t√© intellectuelle
___
## **üéØ Checklist √©thique**
Avant de lancer un scraping, posez-vous ces questions :
### Questions l√©gales
- [ ] Ai-je lu les ToS du site ?
- [ ] Le robots.txt autorise-t-il le scraping ?
- [ ] Existe-t-il une API officielle ?
- [ ] Vais-je scraper des donn√©es personnelles ?
- [ ] Le contenu est-il prot√©g√© par copyright ?
### Questions techniques
- [ ] Mes param√®tres sont-ils respectueux ? (d√©lais, concurrence)
- [ ] Mon User-Agent est-il honn√™te ?
- [ ] Ai-je un syst√®me de retry raisonnable ?
- [ ] Vais-je cacher les requ√™tes ?
- [ ] Puis-je scraper en heures creuses ?
### Questions √©thiques
- [ ] Mon scraping pourrait-il nuire au site ?
- [ ] Puis-je justifier mon objectif ?
- [ ] Les donn√©es seront-elles utilis√©es √† bon escient ?
- [ ] Suis-je transparent sur mon identit√© ?
- [ ] Ai-je contact√© le propri√©taire si n√©cessaire ?

**Si vous r√©pondez "non" ou "je ne sais pas" √† plusieurs questions ‚Üí NE PAS SCRAPER.**
___
## **üìö Ressources suppl√©mentaires**
### **Lectures recommand√©es**
* **RGPD** : https://gdpr.eu/
* **robots.txt RFC** : https://www.rfc-editor.org/rfc/rfc9309
* **Web Scraping Ethics** : https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01
* **Legal aspects** : https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/
### **Alternatives au scraping**
1. **APIs officielles** : Toujours privil√©gier
2. **Open Data** : Donn√©es gouvernementales ouvertes
3. **RSS/Atom feeds** : Pour les actualit√©s
4. **Web Archives** : Archive.org, Common Crawl
5. **Datasets publics** : Kaggle, UCI ML Repository
___
## **üîê Gestion des donn√©es personnelles (RGPD)**
### **Qu'est-ce qu'une donn√©e personnelle ?**
Toute information permettant d'identifier une personne :
* Nom, pr√©nom, email
* Adresse IP, cookies
* Num√©ro de t√©l√©phone
* Photo, voix
* Donn√©es de localisation
### **Obligations RGPD**
Si vous scrapez des donn√©es personnelles :
```
‚úÖ Base l√©gale valide (consentement, int√©r√™t l√©gitime, etc.)
‚úÖ Information transparente des personnes
‚úÖ Droit d'acc√®s, rectification, suppression
‚úÖ S√©curisation des donn√©es
‚úÖ Dur√©e de conservation limit√©e
‚úÖ Pas de transfert hors UE sans garanties
```
**Conseil** : **√âvitez compl√®tement de scraper des donn√©es personnelles** si possible.
___
## **üåü En r√©sum√©**
### **Les 3 piliers du scraping √©thique**
1. **L√âGALIT√â** : Respecter les lois et ToS
2. **RESPECT** : Ne pas nuire au site ou aux utilisateurs
3. **TRANSPARENCE** : √ätre honn√™te sur qui vous √™tes et ce que vous faites
<!-- ### **Le test du journal**
> **"Serais-je √† l'aise si mes pratiques de scraping faisaient la Une d'un journal ?"**

Si la r√©ponse est non, reconsid√©rez votre approche. -->
<!-- ___
## **üìû Contact et signalement**
Si vous constatez une utilisation abusive de ce projet ou avez des pr√©occupations √©thiques :
1. Ouvrir une issue GitHub avec le tag `ethics`
2. Contacter directement le propri√©taire du projet
3. Signaler aux autorit√©s comp√©tentes si n√©cessaire (CNIL en France) -->
___
**Rappel final** : Ce projet est un outil **√©ducatif** pour apprendre Scrapy et MongoDB sur un site de **d√©monstration**. L'utilisation sur des sites r√©els n√©cessite une analyse √©thique et l√©gale approfondie au cas par cas.