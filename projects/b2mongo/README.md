# **Books to MongoDB**<a href="../../"><img align="right" src="../../assets/atomicWebScraping.png" alt="Web scraping" height="64px"></a>
<div align="center">

![Python](https://img.shields.io/badge/Python-3.13-3776AB?style=flat&logo=python&logoColor=white) 
![Scrapy](https://img.shields.io/badge/Scrapy-Web_Scraping-5E8862?style=flat&logo=scrapy&logoColor=white) 
![MongoDB](https://img.shields.io/badge/MongoDB-NoSQL-47A248?style=flat&logo=mongodb&logoColor=white) 
![Pandas](https://img.shields.io/badge/Data_Analysis-Pandas-150458?style=flat&logo=pandas&logoColor=white) 
![License](https://img.shields.io/badge/Usage-Pedagogique-green?style=flat)

</div><hr>

Un scraper  pour extraire et stocker les donnÃ©es de livres depuis [books.toscrape.com](https://books.toscrape.com) dans MongoDB.  
Ce projet permet de scraper automatiquement les informations dÃ©taillÃ©es de milliers de livres (titre, prix, disponibilitÃ©, images, etc.) et de les stocker dans une base MongoDB locale avec tÃ©lÃ©chargement automatique des images de couverture.
---
### âœ¨ Les principals fonctionnalitÃ©s<a href="#"><img align="right" src="../../assets/b2m.png" alt="FelisCrwler" height="256"></a>
* ğŸ“– Scraping de toutes les catÃ©gories de livres
* ğŸ’¾ Stockage automatique dans MongoDB
* ğŸ–¼ï¸ TÃ©lÃ©chargement des images de couverture
* ğŸ” Filtrage par catÃ©gorie
* ğŸ“Š Limitation du nombre de pages
* ğŸ”„ Gestion de la pagination automatique
* âš¡ OptimisÃ© avec indexation MongoDB
* ğŸ“ˆ Statistiques et logs dÃ©taillÃ©s
## **ğŸ“‹ Les prÃ©requis**
* ![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat&logo=python&logoColor=white)
* ![MongoDB](https://img.shields.io/badge/MongoDB-6+_local_(dÃ©velopper_avec_la_version_7)-47A248?style=flat&logo=mongodb&logoColor=white)
* ![Storage](https://img.shields.io/badge/Storage-500MB_pour_les_images-FABC45?style=flat&logo=hard-drive&logoColor=white_images-FABC45?style=flat&logo=hard-drive&logoColor=white)
## **ğŸš€ L'installation**
### 1. Cloner le projet
```sh
git clone <votre-repo>
cd b2mongo
```
### 2. CrÃ©er l'environnement virtuel
```sh
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```
### 3. Installer les dÃ©pendances
```sh
pip install scrapy pymongo pillow
```
### 4. DÃ©marrer MongoDB
```sh
# Linux/Mac
sudo systemctl start mongod

# Windows
net start MongoDB

# VÃ©rifier que MongoDB est accessible
mongosh
```
## **ğŸ“– Utilisation rapide**
### Scraper tous les livres
```sh
scrapy crawl mongo
```
### Scraper une catÃ©gorie spÃ©cifique
```sh
scrapy crawl mongo -a category="Fiction"
scrapy crawl mongo -a category="History"
```
### Limiter le nombre de pages
```sh
scrapy crawl mongo -a max_pages=5
```
### Combiner les options
```sh
scrapy crawl mongo -a category="Science Fiction" -a max_pages=10
```
### Exporter en CSV/JSON
```sh
scrapy crawl mongo -O books.csv
scrapy crawl mongo -O books.json
```
## **ğŸ“Š La structure des donnÃ©es**
Chaque livre contient :
Champ | Type | Description
---|---|---
`title` | string | Titre du livre
`url` | string | URL de la page
`upc` | string | Code produit unique (clÃ©)
`price_excl_tax` | float | Prix HT
`price_incl_tax` | float | Prix TTC
`tax` | float | Montant de la taxe
`availability` | int | Nombre en stock
`rating` | int | Note (1-5)
`category` | string | CatÃ©gorie
`description` | string | Description
`number_of_reviews` | int | Nombre d'avis
`image_url` | string | URL de l'image
`image_path` | string | Chemin local de l'image
## **ğŸ—‚ï¸ La structure du projet**
```
b2mongo/
â”œâ”€â”€ scrapy.cfg              # Configuration Scrapy
â”œâ”€â”€ b2mongo/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py           # DÃ©finition des items BookItem
â”‚   â”œâ”€â”€ middlewares.py     # Middlewares (standard)
â”‚   â”œâ”€â”€ pipelines.py       # MongoDB + Images pipelines
â”‚   â”œâ”€â”€ settings.py        # Configuration globale
â”‚   â”œâ”€â”€ queries.py         # Exemples de requÃªtes MongoDB
â”‚   â””â”€â”€ spiders/
â”‚       â””â”€â”€ mongo.py       # Spider principal
â””â”€â”€ images/                # Images tÃ©lÃ©chargÃ©es (auto-crÃ©Ã©)
```
## **La structure du spyder**
```mermaid
graph TD
    A[ğŸŒ books.toscrape.com] --> B[Scrapy MongoSpider]
    B --> C[ğŸ“„ ItemLoader + Processors]
    C --> D[ğŸ—„ï¸ MongoDBPipeline<br/>upsert par UPC]
    C --> E[ğŸ–¼ï¸ BooksImagesPipeline<br/>TÃ©lÃ©chargement]
    
    D --> F[ğŸ“Š Collection 'books'<br/>Index: upc, category, rating]
    E --> G[ğŸ–¼ï¸ images/ dossier local]
    
    F --> H[ğŸ” queries.py<br/>AgrÃ©gations + Stats]
    H --> I[ğŸ“ˆ Console / Notebook]
    
    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style G fill:#fff3e0
```
### **L'Ã©tat du spyder**
```mermaid
stateDiagram-v2
    [*] --> Init: __init__(category?, max_pages?)
    Init --> Opened: spider_opened()
    Opened --> Parsing: parse() â†’ catÃ©gories
    Parsing --> Category: parse_category()
    
    Category --> Book: parse_book() â†’ ItemLoader
    Book --> Processing: yield BookItem()
    
    Category --> LimitCheck: max_pages atteint?
    LimitCheck --> Stop: Oui
    LimitCheck --> NextPage: Non â†’ pagination
    
    Processing --> MongoDB: MongoDBPipeline
    Processing --> Images: BooksImagesPipeline
    
    MongoDB --> Closed: close_spider()
    Images --> Closed
    NextPage --> Category
    
    Stop --> Closed
    Closed --> [*]
    
    note right of LimitCheck
        pages_scraped += 1
        books_scraped += 1
    end note
```
### **Les flux de donnÃ©es du spider**
```mermaid
sequenceDiagram
    participant S as MongoSpider
    participant H as books.toscrape.com
    participant I as ItemLoader
    participant P as MongoDBPipeline
    participant M as MongoDB
    
    S->>H: GET / (page d'accueil)
    H->>S: HTML catÃ©gories
    S->>S: parse() â†’ liste catÃ©gories
    loop Pour chaque catÃ©gorie
        S->>H: GET /category/...
        H->>S: HTML livres + pagination
        S->>S: parse_category()
        
        loop Pour chaque livre
            S->>H: GET /catalogue/book/...
            H->>S: HTML dÃ©tails livre
            S->>I: parse_book() â†’ BookItem
            I->>P: item_completed()
        end
        
        S->>H: GET next page?
    end
    
    P->>M: upsert({'upc': ...})
    Note over P,M: Index unique UPC
```
## **ğŸ“Š Diagrammes et architecture**
Pour une comprÃ©hension visuelle du projet :
- **[Architecture complÃ¨te](ARCHITECTURE.md)** - Documentation technique avec diagrammes intÃ©grÃ©s
- **[Collection de diagrammes](DIAGRAMS.md)** - Tous les diagrammes en un seul endroit
- **[RequÃªtes MongoDB](USAGE.md#requÃªtes-mongodb)** - Diagrammes des agrÃ©gations
## **âš™ï¸ La configuration**
### **MongoDB (`settings.py`)**
```py
MONGO_URI = 'mongodb://localhost:27017/'
MONGO_DATABASE = 'books_toscrape'
```
### **Politesse du scraping**
```py
DOWNLOAD_DELAY = 0.5                   # DÃ©lai entre requÃªtes
CONCURRENT_REQUESTS_PER_DOMAIN = 2    # RequÃªtes simultanÃ©es
ROBOTSTXT_OBEY = True                 # Respecter robots.txt
```
### **Pipelines actifs**
```py
ITEM_PIPELINES = {
    'b2mongo.pipelines.BooksImagesPipeline': 200,  # TÃ©lÃ©chargement images
    'b2mongo.pipelines.MongoDBPipeline': 300,      # Stockage MongoDB
}
```
## **ğŸ” Les requÃªtes MongoDB**
Pour les exemples de requÃªtes voir `queries.py` :
```py
from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/')
db = client['books_toscrape']
books = db['books']

# Livres les mieux notÃ©s
top_rated = books.find({'rating': 5}).limit(10)

# Prix moyen par catÃ©gorie
pipeline = [
    {'$group': {
        '_id': '$category',
        'avg_price': {'$avg': '$price_incl_tax'},
        'count': {'$sum': 1}
    }},
    {'$sort': {'avg_price': -1}}
]
results = books.aggregate(pipeline)

# Recherche full-text
books.create_index([('title', 'text'), ('description', 'text')])
results = books.find({'$text': {'$search': 'python programming'}})
```
## **ğŸ“š La documentation complÃ¨te**
* **[USAGE.md](USAGE.md)** - Guide d'utilisation dÃ©taillÃ©
* **[ARCHITECTURE.md](ARCHITECTURE.md)** - Documentation technique
* **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - RÃ©solution de problÃ¨mes
* **[ETHICS.md](ETHICS.md)** - Ã‰thique et bonnes pratiques
* **[CONTRIBUTING.md](CONTRIBUTING.md)** - Guide pour contributeurs
## **âš–ï¸ Ã‰thique et lÃ©galitÃ©**
Ce projet est conÃ§u **uniquement Ã  des fins Ã©ducatives** sur le site de dÃ©monstration [books.toscrape.com](https://books.toscrape.com), crÃ©Ã© spÃ©cifiquement pour l'apprentissage du scraping.

âš ï¸ **Important** : Ne pas utiliser ce code sur des sites de production sans permission. Voir [ETHICS.md](ETHICS.md) pour plus de dÃ©tails.
## **ğŸ“ Licence**
MIT License - Voir LICENSE pour plus de dÃ©tails.
___
> **NOTA**  
>  Ce projet respecte les rÃ¨gles du `robots.txt` et implÃ©mente des dÃ©lais entre requÃªtes pour un scraping responsable.